Subject: Notice of New Feature Implementation & Preliminary Cost Estimate

Hello [Project Manager Name],

I hope you’re doing well. I want to give you a heads-up about a new feature we need to implement, which involves changes to how we retrieve position error data from our service provider. Previously, we used a single API call to get all position error details. Now, the provider has split the workflow into multiple endpoints and parquet files, requiring us to do extra integration steps and data merging.


---

What’s Changing

1. Multiple Endpoints:

One endpoint returns only the list of positions that are in error.

Another endpoint gives us a list of parquet files for a particular job.

A final endpoint serves up those parquet files.



2. Parquet File Processing:

We will need to download, parse, and merge data from these parquet files to retrieve the full error details for each position.

Each parquet file can be around 50MB in size, so we have to handle memory usage and possibly cache or persist data.



3. New Logic & Data Aggregation:

We’ll need to fetch positions in error, retrieve the list of files, parse them, and then merge the resulting data with the initial list of positions.





---

Work Involved & Preliminary Effort Estimate

Requirements & Design (1–2 days)
Clarifying data formats, deciding on how we store and merge parquet data, and creating a brief technical design.

Integration of New Endpoints (1–2 days)
Writing Spring services/clients to call the new endpoints reliably, with proper error handling.

Parquet Parsing & Data Aggregation (2–4 days)
Choosing a parquet library, parsing 50MB files, deciding on whether to store data in-memory or use a cache.

Merging & Business Logic (1–2 days)
Mapping each position to its error details, handling edge cases or missing data.

Performance & Resilience (1–2 days)
Addressing concurrency, memory usage, and caching strategies to handle files of this size.

Testing & Deployment (1–2 days)
Integration tests, user acceptance, and final rollout.


Overall, we anticipate roughly 7–13 days of active development (about 2–3 weeks of calendar time if one developer is assigned) to design, implement, test, and deploy this feature. This estimate includes time for potential performance tuning and handling any edge cases.


---

Potential Costs

If we assume a typical daily rate for a developer (or the internal equivalent cost to the project), the cost would align with (7–13 days) × (Daily Rate). This cost can vary based on factors such as:

Additional overhead if more than one developer is involved.

Unforeseen complexities with large file handling or integration issues.

Changes in project scope or requirement clarifications.



---

Risks & Recommendations

Memory Usage: 50MB parquet files are not huge, but we should be mindful if multiple files are processed in parallel.

Caching Strategy: If these files will be accessed repeatedly, a caching layer (like Redis) or short-term storage might be necessary.

Timeline Flexibility: We recommend a 20–30% contingency for unexpected issues, especially if the service provider’s APIs or data format change again.



---

Next Steps

I’ll finalize the technical design and confirm any outstanding questions with the provider.

Please let me know if you have any questions, or if you’d like to schedule a brief walkthrough of the plan.

Once approved, we can allocate the necessary development resources and begin implementation.


Thank you for your attention to this matter, and I look forward to your feedback.


---

Best Regards,
[Your Name]
[Your Title / Role]
[Project / Team Name]
[Contact Information]




Subject: Update on New Error Data Retrieval Feature & Storage Considerations

Hello [Project Manager Name],

I hope you’re doing well. I’d like to update you on the new feature implementation regarding how we retrieve error details for our risk positions, and highlight some concerns about file storage as we’ll be handling multiple jobs daily.


---

1. Summary of the New Workflow

Multiple Endpoints:
Our provider now supplies separate endpoints:

1. One returns the list of positions in error.


2. Another provides a list of Parquet files for a given job.


3. A third allows us to download each file.



Parquet Files (~50 MB each):
We need to parse and merge data from these files to fully retrieve error details per position.

Frequent Jobs:
Because we process many jobs per day, we could be dealing with several 50 MB files regularly.



---

2. Storage & Disk Usage Considerations

If we store all these Parquet files on local disk for every job, we’ll risk running into significant storage overhead. We have a few strategies to manage this:

1. Ephemeral Storage:

Download, parse, extract the relevant data, and delete the raw Parquet files.

This prevents disk usage from accumulating but means we won’t keep the original files unless explicitly needed for audits.



2. External Object Storage (e.g., S3/Blob):

If we need raw files for long-term reference, we can offload them to a cloud service.

Reduces local disk usage, but adds some operational overhead.



3. Persist Only Relevant Data:

Parse the Parquet files and store only the needed information in a database.

Ensures quick lookups without re-parsing.

We delete the full files once processing is done.




We’ll choose the most efficient approach based on whether we need to reprocess old data frequently or store raw files for compliance. Our initial inclination is to process and discard, as long as the provider can re-supply files if we ever need them again.


---

3. Effort & Cost Estimates

Implementing this new workflow involves:

1. Requirements & Design (1–2 days)


2. Endpoint Integration (1–2 days)


3. Parquet Parsing & Aggregation (2–4 days)


4. Merging & Business Logic (1–2 days)


5. Performance, Resilience (1–2 days)


6. Testing & Deployment (1–2 days)



That brings us to roughly 7–13 days of development time, or about 2–3 weeks on the calendar if a single developer is focused on it. The final cost will depend on our standard daily rate multiplied by the total days, plus a 20–30% buffer for unforeseen complexities.


---

4. Next Steps & Approval

Confirm Storage Approach: Decide whether we adopt ephemeral storage, external object storage, or a database approach for the files.

Finalize Design: Once we confirm the storage strategy, I can finalize the technical plan and integration details.

Implementation & Testing: We’ll proceed with development, ensure performance testing, and handle any scalability concerns.


Please let me know if you have any questions or would like to discuss the storage options in more detail. Once we have your go-ahead on the approach, we can begin implementation.

Thank you for your time, and I look forward to your feedback.


---

Best Regards,
[Your Name]
[Your Role / Team]
[Your Contact Information]




SELECT
  project_id,
  FORMAT_TIMESTAMP('%Y-%m', creation_time) AS usage_month,
  user_email,
  ROUND(SUM(bytes_processed) / (1024 * 1024 * 1024), 2) AS gb_processed,
  ROUND(SUM(bytes_processed) / (1024 * 1024 * 1024 * 1024) * 6.25, 2) AS total_cost_usd
FROM
  `dev_project.region-eu.INFORMATION_SCHEMA.JOBS_BY_PROJECT`
WHERE
  job_type = "QUERY"
  AND state = "DONE"
  AND creation_time >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 3 MONTH)
GROUP BY
  project_id, usage_month, user_email

UNION ALL

SELECT
  project_id,
  FORMAT_TIMESTAMP('%Y-%m', creation_time) AS usage_month,
  user_email,
  ROUND(SUM(bytes_processed) / (1024 * 1024 * 1024), 2) AS gb_processed,
  ROUND(SUM(bytes_processed) / (1024 * 1024 * 1024 * 1024) * 6.25, 2) AS total_cost_usd
FROM
  `uat_project.region-eu.INFORMATION_SCHEMA.JOBS_BY_PROJECT`
WHERE
  job_type = "QUERY"
  AND state = "DONE"
  AND creation_time >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 3 MONTH)
GROUP BY
  project_id, usage_month, user_email;

"Listing directory before deletion:"
                                ls -la /opt/risktore/maven/repository/log4j/log4j || echo "Directory not found."

pipeline {
    agent none
    stages {
        stage('Cleanup Old Log4j on All Nodes') {
            steps {
                script {
                    def nodesWithLabel = []
                    // Find all nodes with the label
                    node('your-slave-label') {
                        nodesWithLabel = env.NODE_NAME
                    }
                    // Iterate over the nodes
                    for (nodeName in nodesWithLabel) {
                        node(nodeName) {
                            echo "Cleaning up on node: ${nodeName}"
                            sh '''
                                if [ -d /opt/risktore/maven/repository/log4j/log4j/1.12.12 ]; then
                                    rm -rf /opt/risktore/maven/repository/log4j/log4j/1.12.12
                                    echo "Old Log4j dependency deleted on ${NODE_NAME}."
                                else
                                    echo "Log4j dependency not found or already deleted on ${NODE_NAME}."
                                fi
                            '''
                        }
                    }
                }
            }
        }
    }
}



SELECT
  user_email,
  query,
  ROUND(bytes_processed / (1024 * 1024 * 1024), 2) AS gb_processed,
  ROUND((bytes_processed / (1024 * 1024 * 1024 * 1024)) * 6.25, 2) AS estimated_cost_usd,
  start_time,
  job_type
FROM
  `region-eu.INFORMATION_SCHEMA.JOBS_BY_PROJECT`
WHERE
  job_type = "QUERY"
  AND state = "DONE"
  AND creation_time BETWEEN TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 30 DAY) AND CURRENT_TIMESTAMP()
ORDER BY
  estimated_cost_usd DESC
LIMIT 100;


// Input variables
var rollDate = 20230101;  // Replace with your target roll date
var processIds = ["PROCESS123", "PROCESS456"];  // Replace with your process IDs

// Backup collections
var jobStatusEventsBackup = "JobStatusEvents_Backup";
var jobStatusBackup = "JobStatus_Backup";
var fileStatusBackup = "FileStatus_Backup";

// Step 1: Backup `JobStatusEvents`
print("Backing up JobStatusEvents...");
db.JobStatusEvents.aggregate([
    {
        $match: {
            rolldate: rollDate,
            processId: { $in: processIds }
        }
    },
    {
        $merge: {
            into: jobStatusEventsBackup,
            whenMatched: "merge", // Merge if already exists
            whenNotMatched: "insert" // Insert new documents
        }
    }
]);
print("Backup for JobStatusEvents completed.");

// Step 2: Backup `JobStatus`
print("Backing up JobStatus...");
db.JobStatus.aggregate([
    {
        $match: {
            rolldate: rollDate,
            processId: { $in: processIds }
        }
    },
    {
        $merge: {
            into: jobStatusBackup,
            whenMatched: "merge",
            whenNotMatched: "insert"
        }
    }
]);
print("Backup for JobStatus completed.");

// Step 3: Backup `FileStatus`
print("Backing up FileStatus...");
db.FileStatus.aggregate([
    {
        $match: {
            rolldate: rollDate,
            processId: { $in: processIds }
        }
    },
    {
        $merge: {
            into: fileStatusBackup,
            whenMatched: "merge",
            whenNotMatched: "insert"
        }
    }
]);
print("Backup for FileStatus completed.");

// Step 4: Delete from `JobStatusEvents`
print("Deleting from JobStatusEvents...");
var eventsResult = db.JobStatusEvents.deleteMany({
    rolldate: rollDate,
    processId: { $in: processIds }
});
print("Deleted records from JobStatusEvents: " + eventsResult.deletedCount);

// Step 5: Delete from `JobStatus`
print("Deleting from JobStatus...");
var jobResult = db.JobStatus.deleteMany({
    rolldate: rollDate,
    processId: { $in: processIds }
});
print("Deleted records from JobStatus: " + jobResult.deletedCount);

// Step 6: Delete from `FileStatus`
print("Deleting from FileStatus...");
var fileResult = db.FileStatus.deleteMany({
    rolldate: rollDate,
    processId: { $in: processIds }
});
print("Deleted records from FileStatus: " + fileResult.deletedCount);

print("Rollback script completed successfully. Backups are available for recovery if needed.");




// Recover `JobStatusEvents`
print("Recovering JobStatusEvents...");
db.JobStatusEvents_Backup.aggregate([
    {
        $merge: {
            into: "JobStatusEvents",
            whenMatched: "merge",
            whenNotMatched: "insert"
        }
    }
]);
print("Recovery for JobStatusEvents completed.");

// Recover `JobStatus`
print("Recovering JobStatus...");
db.JobStatus_Backup.aggregate([
    {
        $merge: {
            into: "JobStatus",
            whenMatched: "merge",
            whenNotMatched: "insert"
        }
    }
]);
print("Recovery for JobStatus completed.");

// Recover `FileStatus`
print("Recovering FileStatus...");
db.FileStatus_Backup.aggregate([
    {
        $merge: {
            into: "FileStatus",
            whenMatched: "merge",
            whenNotMatched: "insert"
        }
    }
]);
print("Recovery for FileStatus completed.");

type(s).

jobType (array of strings): Job type categories to filter by.





3. Logic:

Latest Version: Automatically retrieve only the latest version of each job, using a version or updatedAt field to identify the most recent.

Filtering: Limit filtering to the specified fields (rollDate date range, jobId, and jobType) for simplicity.

Pagination and Sorting:

Include limit and page query parameters for pagination.

Default sorting by updatedAt in descending order.




4. Swagger UI Integration:

Interactive Documentation: Document the API in Swagger, specifying all parameters and their usage.

Examples: Provide sample requests in Swagger to illustrate different filter combinations.

Parameter Descriptions: Clearly document the purpose of each parameter.



5. Response:

Format: JSON array of job objects.

Fields: Return all fields in each job document.

Error Handling: Implement error handling for invalid parameters or query formats.



6. Example GET Query:

Example: Retrieve active jobs of types "engineering" and "management" within a specific date range, limiting results to the latest version:

GET /jobs?startDate=2024-01-01&endDate=2024-12-31&jobId=123,456&jobType=engineering,management&limit=10&page=1





---

Acceptance Criteria:

1. The /jobs endpoint retrieves only the latest version of each job, based on the updatedAt or version field.


2. Filters for rollDate (using startDate and endDate), jobId (single or multiple), and jobType (array) are functional and documented in Swagger.


3. The API includes pagination (limit and page parameters) and default sorting by updatedAt in descending order.


4. Swagger UI is configured with parameter descriptions and usage examples.


5. Response is formatted in JSON, containing all job fields by default.


6. Error handling is implemented for invalid or missing parameters.




---

Priority:

High

Story Points:

5


---

This refined approach simplifies the filters and keeps the API focused on core needs, making it more efficient to implement and easier for clients to use. Let me know if there are any further adjustments needed!


<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Query Builder</title>
    
    <!-- jQuery CDN -->
    <script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>
    
    <!-- jQuery QueryBuilder CSS and JS -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/jQuery-QueryBuilder/dist/css/query-builder.default.min.css">
    <script src="https://cdn.jsdelivr.net/npm/jQuery-QueryBuilder/dist/js/query-builder.standalone.min.js"></script>

    <style>
        /* Custom CSS for layout */
        body {
            font-family: Arial, sans-serif;
            margin: 0;
            padding: 0;
            display: flex;
            flex-direction: column;
            height: 100vh;
        }
        #builder {
            flex: 1;
            padding: 20px;
            background-color: #f8f9fa;
        }
        #json-display {
            height: 200px;
            padding: 20px;
            background-color: #ffffff;
            border-top: 1px solid #ddd;
            position: relative;
        }
        #json-output {
            width: 100%;
            height: 150px;
            font-family: monospace;
            border: 1px solid #ccc;
            padding: 10px;
            box-sizing: border-box;
            resize: none;
        }
        #copy-button {
            position: absolute;
            right: 20px;
            bottom: 10px;
            padding: 8px 12px;
            background-color: #007bff;
            color: white;
            border: none;
            border-radius: 5px;
            cursor: pointer;
        }
    </style>
</head>
<body>

    <!-- Query Builder Section -->
    <div id="builder"></div>

    <!-- JSON Output and Copy Button Section -->
    <div id="json-display">
        <textarea id="json-output" readonly></textarea>
        <button id="copy-button">Copy to Clipboard</button>
    </div>

    <script>
        // Initialize the Query Builder with basic fields
        $('#builder').queryBuilder({
            filters: [
                {
                    id: 'jobIds',
                    label: 'Job IDs',
                    type: 'string',
                    input: 'text',
                    operators: ['equal', 'not_equal', 'in', 'not_in'],
                },
                {
                    id: 'field',
                    label: 'Field',
                    type: 'string',
                    input: 'select',
                    values: {
                        'Result_snap': 'Result_snap',
                        'Result_context': 'Result_context',
                        'Result_EPI': 'Result_EPI',
                        'Result_HMSBook': 'Result_HMSBook',
                        'Result_JobId': 'Result_JobId',
                        'Result_InstrumentId': 'Result_InstrumentId',
                        'Result_Errors': 'Result_Errors'
                    },
                    operators: ['equal', 'not_equal']
                },
                {
                    id: 'aggFunc',
                    label: 'Aggregation Function',
                    type: 'string',
                    input: 'select',
                    values: {
                        'sum': 'sum',
                        'avg': 'avg',
                        'min': 'min',
                        'max': 'max',
                        'count': 'count',
                        'any_value': 'any_value'
                    }
                },
                {
                    id: 'pivot',
                    label: 'Pivot',
                    type: 'string',
                    operators: ['equal', 'not_equal']
                }
            ]
        });

        // Update JSON output whenever the query is updated
        $('#builder').on('afterUpdateRuleValue afterUpdateRuleFilter afterDeleteRule afterCreateRule', function() {
            var rules = $('#builder').queryBuilder('getRules');
            if (!$.isEmptyObject(rules)) {
                $('#json-output').val(JSON.stringify(rules, null, 2));
            } else {
                $('#json-output').val('');
            }
        });

        // Copy JSON to Clipboard
        $('#copy-button').on('click', function() {
            const jsonOutput = document.getElementById('json-output');
            jsonOutput.select();
            document.execCommand('copy');
            alert('JSON copied to clipboard!');
        });
    </script>

</body>
</html>



openapi: 3.0.1
info:
  title: Warehouse API
  version: 1.0.0
paths:
  /warehouse:
    post:
      summary: Submit a warehouse request
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/WarehouseRequest'
            examples:
              example1:
                summary: Example Request
                value:
                  jobIds:
                    - "2ab3909d-f423-4d61-aa94-30d8f0a6d918"
                  valueCols:
                    - field: "Result_snap"
                      aggFunc: "sum"
                    - field: "Result_context"
                      aggFunc: "avg"
                  pivot:
                    aggColumn: "Result_Exposure_Value"
                    pivotColumn: "Result_MultipriceStep"
                    pivotValues:
                      - "RollBasedPnL"
                      - "PortfolioPnL"
      responses:
        '200':
          description: Successful response
components:
  schemas:
    WarehouseRequest:
      type: object
      properties:
        jobIds:
          type: array
          items:
            type: string
          description: List of Job IDs
        valueCols:
          type: array
          items:
            $ref: '#/components/schemas/ColumnVO'
          description: List of value columns
        aggregation:
          type: array
          items:
            type: string
            enum:
              - Result_EPI
              - Result_Exposure_RiskFactor1
              - Result_Exposure_RiskFactor2
              - Result_Exposure_InstrumentId
          description: Aggregation columns
        pivot:
          $ref: '#/components/schemas/Pivot'
        filterModel:
          type: object
          additionalProperties:
            $ref: '#/components/schemas/ColumnFilter'
          description: Filter model for additional query filters
      required:
        - jobIds
        - valueCols
    ColumnVO:
      type: object
      properties:
        field:
          type: string
          enum:
            - Result_snap
            - Result_context
            - Result_EPI
            - Result_HMSBook
            - Result_JobId
            - Result_InstrumentId
            - Result_Errors
            - Result_PortfolioId
            - Result_LegalEntity
            - Result_Desk
          description: Field name
        aggFunc:
          type: string
          enum:
            - sum
            - avg
            - min
            - max
            - count
            - any_value
          description: Aggregation function (required if 'aggregation' exists)
    Pivot:
      type: object
      properties:
        aggColumn:
          type: string
          description: Aggregation column
        pivotColumn:
          type: string
          description: Pivot column
        pivotValues:
          type: array
          items:
            type: string
            enum:
              - RollBasedPnL
              - PortfolioPnL
              - HypoPnL
              - UnexplainedHypo
              - ThetaPnL
              - FxSpotPnL
          description: Pivot values
      required:
        - aggColumn
        - pivotColumn
        - pivotValues
    ColumnFilter:
      type: object
      properties:
        filterType:
          type: string
          enum:
            - set
            - range
            - date
            - text
          description: Type of filter to apply
        values:
          type: array
          items:
            type: string
          description: Values to filter by (for set-type filters)
        range:
          type: object
          properties:
            start:
              type: string
              format: date-time
              description: Start of the range (for range-type filters)
            end:
              type: string
              format: date-time
              description: End of the range (for range-type filters)
        text:
          type: string
          description: Text-based filter input
      description: Column filter options for filtering query results


openapi: 3.0.1
info:
  title: Warehouse API
  version: 1.0.0
paths:
  /warehouse:
    post:
      summary: Submit a warehouse request
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/WarehouseRequest'
      responses:
        '200':
          description: Successful response
components:
  schemas:
    WarehouseRequest:
      type: object
      properties:
        jobIds:
          type: array
          items:
            type: string
          description: List of Job IDs
        valueCols:
          type: array
          items:
            $ref: '#/components/schemas/ColumnVO'
          description: List of value columns
        aggregation:
          type: array
          items:
            type: string
            enum:
              - Result_EPI
              - Result_Exposure_RiskFactor1
              - Result_Exposure_RiskFactor2
              - Result_Exposure_InstrumentId
          description: Aggregation columns
        filterModel:
          type: object
          additionalProperties:
            $ref: '#/components/schemas/ColumnFilter'
        pivot:
          $ref: '#/components/schemas/Pivot'
        distinct:
          type: boolean
          default: false
        exportFormat:
          type: string
          enum:
            - avro
            - json
            - csv
          default: avro
      required:
        - jobIds
        - valueCols
      allOf:
        # Conditional validation: If 'aggregation' is present and not empty, then 'aggFunc' in 'valueCols' is required
        - if:
            properties:
              aggregation:
                minItems: 1
          then:
            properties:
              valueCols:
                type: array
                items:
                  $ref: '#/components/schemas/ColumnVOWithAggFunc'
    ColumnVO:
      type: object
      properties:
        field:
          type: string
          description: Field name
          enum:
            - Result_snap
            - Result_context
            - Result_EPI
            - Result_HMSBook
            - Result_JobId
            - Result_InstrumentId
            - Result_Errors
            - Result_PortfolioId
            - Result_LegalEntity
            - Result_Desk
        displayName:
          type: string
          description: Display name
    ColumnVOWithAggFunc:
      allOf:
        - $ref: '#/components/schemas/ColumnVO'
        - type: object
          properties:
            aggFunc:
              type: string
              description: Aggregation function
              enum:
                - sum
                - avg
                - min
                - max
                - count
                - any_value
          required:
            - aggFunc
    Pivot:
      type: object
      properties:
        aggColumn:
          type: string
          description: Aggregation column
        pivotColumn:
          type: string
          description: Pivot column
        pivotValues:
          type: array
          items:
            type: string
            enum:
              - RollBasedPnL
              - PortfolioPnL
              - HypoPnL
              - UnexplainedHypo
              - ThetaPnL
              - FxSpotPnL
      required:
        - aggColumn
        - pivotColumn
        - pivotValues


Sure! Here are the detailed examples for parts 4, 5, and 6 of the client's documentation.

### 4. Building a Query in JSON

**Structure of the JSON Request**
- The JSON request structure is designed to build SQL queries dynamically. Below is the general structure:

```json
{
  "noPagination": false,
  "pageNumber": 1,
  "pageSize": 10,
  "rowGroupCols": [
    {"field": "tablePrefix.columnName", "displayName": "columnAlias", "aggFunc": "aggregationFunction"}
  ],
  "valueCols": [
    {"field": "tablePrefix.columnName", "displayName": "columnAlias", "aggFunc": "aggregationFunction"}
  ],
  "selectCols": ["tablePrefix.columnName"],
  "groupKeys": [],
  "pivotColumn": "tablePrefix.columnName",
  "pivot": {
    "aggColumn": "tablePrefix.columnName",
    "pivotColumn": "tablePrefix.columnName",
    "pivotValues": ["value1", "value2"]
  },
  "filterModel": {
    "tablePrefix.columnName": {"values": ["value1", "value2"], "filterType": "set"}
  },
  "sortModel": [
    {"colId": "tablePrefix.columnName", "sort": "asc"}
  ]
}
```

**Detailed Explanation of Each JSON Field**

- `noPagination`: Boolean to disable pagination.
- `pageNumber`: The page number for pagination.
- `pageSize`: The size of each page.
- `rowGroupCols`: List of columns to group by. Each item should have:
  - `field`: The column name prefixed with the table alias.
  - `displayName`: An alias for the column in the results.
  - `aggFunc`: The aggregation function to apply (e.g., `SUM`, `COUNT`).
- `valueCols`: List of columns to aggregate. Each item should have:
  - `field`: The column name prefixed with the table alias.
  - `displayName`: An alias for the column in the results.
  - `aggFunc`: The aggregation function to apply.
- `selectCols`: List of columns to select.
- `groupKeys`: List of group keys.
- `pivotColumn`: Column to pivot on.
- `pivot`: An object containing:
  - `aggColumn`: Column to aggregate in the pivot.
  - `pivotColumn`: Column to pivot on.
  - `pivotValues`: List of values to pivot.
- `filterModel`: A map of filters. Each filter should have:
  - `values`: List of values to filter.
  - `filterType`: The type of filter (`set`, `number`, etc.).
- `sortModel`: List of sorting instructions. Each item should have:
  - `colId`: The column to sort by.
  - `sort`: The sort direction (`asc`, `desc`).

**Example JSON Queries**

1. **Basic Query**

```json
{
  "noPagination": false,
  "pageNumber": 1,
  "pageSize": 10,
  "rowGroupCols": [
    {"field": "cust.customerId", "displayName": "customerId", "aggFunc": null}
  ],
  "valueCols": [
    {"field": "ord.totalAmount", "displayName": "totalAmount", "aggFunc": "SUM"}
  ],
  "selectCols": ["cust.customerId", "ord.totalAmount"],
  "groupKeys": [],
  "filterModel": {
    "cust.customerId": {"values": ["123", "456"], "filterType": "set"}
  },
  "sortModel": [
    {"colId": "cust.customerId", "sort": "asc"}
  ]
}
```

2. **Pivot Query**

```json
{
  "noPagination": false,
  "pageNumber": 1,
  "pageSize": 10,
  "rowGroupCols": [
    {"field": "cust.customerId", "displayName": "customerId", "aggFunc": null},
    {"field": "ord.orderId", "displayName": "orderId", "aggFunc": null}
  ],
  "valueCols": [
    {"field": "ord.totalAmount", "displayName": "totalAmount", "aggFunc": "SUM"}
  ],
  "selectCols": ["cust.customerId", "ord.orderId", "ord.totalAmount"],
  "groupKeys": [],
  "pivotColumn": "ord.orderStatus",
  "pivot": {
    "aggColumn": "ord.totalAmount",
    "pivotColumn": "ord.orderStatus",
    "pivotValues": ["completed", "pending"]
  },
  "filterModel": {
    "cust.customerId": {"values": ["123", "456"], "filterType": "set"},
    "ord.totalAmount": {"filter": 100, "type": "greaterThan"}
  },
  "sortModel": [
    {"colId": "cust.customerId", "sort": "asc"}
  ]
}
```

### 5. API Endpoints

**List of Available Endpoints**

- **POST /api/v1/query**
  - Description: Generates a SQL query based on the JSON input.
  - Request Body: JSON object with query parameters.
  - Response: JSON object with the generated SQL query.

**Detailed Explanation of Each Endpoint**

**POST /api/v1/query**

- **URL**: `/api/v1/query`
- **HTTP Method**: `POST`
- **Description**: This endpoint generates a SQL query based on the JSON input provided.
- **Request Body**: The JSON request body should follow the structure described in the "Building a Query in JSON" section.
- **Response**:
  - `query`: The generated SQL query.

**Example API Requests and Responses**

**Request:**

```http
POST /api/v1/query
Content-Type: application/json

{
  "noPagination": false,
  "pageNumber": 1,
  "pageSize": 10,
  "rowGroupCols": [
    {"field": "cust.customerId", "displayName": "customerId", "aggFunc": null},
    {"field": "ord.orderId", "displayName": "orderId", "aggFunc": null}
  ],
  "valueCols": [
    {"field": "ord.totalAmount", "displayName": "totalAmount", "aggFunc": "SUM"}
  ],
  "selectCols": ["cust.customerId", "ord.orderId", "ord.totalAmount"],
  "groupKeys": [],
  "pivotColumn": "ord.orderStatus",
  "pivot": {
    "aggColumn": "ord.totalAmount",
    "pivotColumn": "ord.orderStatus",
    "pivotValues": ["completed", "pending"]
  },
  "filterModel": {
    "cust.customerId": {"values": ["123", "456"], "filterType": "set"},
    "ord.totalAmount": {"filter": 100, "type": "greaterThan"}
  },
  "sortModel": [
    {"colId": "cust.customerId", "sort": "asc"}
  ]
}
```

**Response:**

```json
{
  "query": "SELECT cust.customerId, ord.orderId, SUM(ord.totalAmount) as totalAmount FROM Customers AS cust LEFT JOIN UNNEST(cust.orders) AS ord WHERE cust.customerId IN ('123', '456') AND ord.totalAmount > 100 GROUP BY cust.customerId, ord.orderId ORDER BY cust.customerId asc PIVOT (SUM(ord.totalAmount) FOR ord.orderStatus IN ('completed', 'pending'))"
}
```

### 6. Common Use Cases

**Examples of Common Queries**

1. **Filtering Data**
   - Filter customers with specific IDs and sum their order amounts.

**Example JSON:**
```json
{
  "noPagination": false,
  "pageNumber": 1,
  "pageSize": 10,
  "rowGroupCols": [
    {"field": "cust.customerId", "displayName": "customerId", "aggFunc": null}
  ],
  "valueCols": [
    {"field": "ord.totalAmount", "displayName": "totalAmount", "aggFunc": "SUM"}
  ],
  "selectCols": ["cust.customerId", "ord.totalAmount"],
  "groupKeys": [],
  "filterModel": {
    "cust.customerId": {"values": ["123", "456"], "filterType": "set"}
  },
  "sortModel": [
    {"colId": "cust.customerId", "sort": "asc"}
  ]
}
```

**Example SQL:**
```sql
SELECT cust.customerId, SUM(ord.totalAmount) as totalAmount
FROM Customers AS cust
LEFT JOIN UNNEST(cust.orders) AS ord
WHERE cust.customerId IN ('123', '456')
GROUP BY cust.customerId
ORDER BY cust.customerId asc
```

2. **Grouping Data**
   - Group orders by customer and order ID, and sum the total amount.

**Example JSON:**
```json
{
  "noPagination": false,
  "pageNumber": 1,
  "pageSize": 10,
  "rowGroupCols": [
    {"field": "cust.customerId", "displayName": "customerId", "aggFunc": null},
    {"field": "ord.orderId", "displayName": "orderId", "aggFunc": null}
  ],
  "valueCols": [
    {"field": "ord.totalAmount", "displayName": "totalAmount", "aggFunc": "SUM"}
  ],
  "selectCols": ["cust.customerId", "ord.orderId", "ord.totalAmount"],
  "groupKeys
