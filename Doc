package com.example.service;

import org.bson.Document;
import org.junit.jupiter.api.BeforeEach;
import org.junit.jupiter.api.Test;
import org.junit.jupiter.params.ParameterizedTest;
import org.junit.jupiter.params.provider.NullAndEmptySource;
import org.junit.jupiter.params.provider.ValueSource;
import org.mockito.Mock;
import org.mockito.MockitoAnnotations;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.boot.test.autoconfigure.web.client.RestClientTest;
import org.springframework.http.HttpStatus;
import org.springframework.http.MediaType;
import org.springframework.test.web.client.MockRestServiceServer;
import org.springframework.web.client.RestClient;

import static org.assertj.core.api.Assertions.assertThat;
import static org.junit.jupiter.api.Assertions.assertThrows;
import static org.mockito.Mockito.*;
import static org.springframework.test.web.client.match.MockRestRequestMatchers.*;
import static org.springframework.test.web.client.response.MockRestResponseCreators.*;

@RestClientTest(VaultRestService.class)
public class VaultRestServiceTest {

    @Autowired
    private RestClient restClient;

    @Autowired
    private MockRestServiceServer server;

    @Mock
    private PropertiesConfig propertiesConfig;

    @Mock
    private VaultToken vaultToken;

    @Mock
    private VaultTokenRequester vaultTokenRequester;

    private VaultRestService vaultRestService;

    @BeforeEach
    public void setUp() {
        MockitoAnnotations.openMocks(this);
        
        // Setup mock configurations
        when(propertiesConfig.getOauthUrl()).thenReturn("http://oauth-server.com");
        when(propertiesConfig.getOauthClientId()).thenReturn("client-id");
        when(propertiesConfig.getOauthClientSecret()).thenReturn("client-secret");
        when(propertiesConfig.getVaultServiceUser()).thenReturn("service-user");
        when(propertiesConfig.getVaultServicePassword()).thenReturn("service-password");
        when(propertiesConfig.getVaultServiceApiRetry()).thenReturn(3);
        
        // URLs for API calls
        when(propertiesConfig.getErrorPositionUrl()).thenReturn("http://api.example.com/error/{job}/{riskSourceIds}/{jobUUID}/{cf}");
        when(propertiesConfig.getParquetFileListingsUrl()).thenReturn("http://api.example.com/parquet/{job}/{jobUUID}");
        when(propertiesConfig.getDownloadParquetFileUrl()).thenReturn("http://api.example.com/download/{bucketName}/{objectName}");
        
        // Create service with mocked dependencies
        vaultRestService = new VaultRestService(propertiesConfig, restClient);
        
        // Manually set the token requester and token
        try {
            java.lang.reflect.Field tokenRequesterField = VaultRestService.class.getDeclaredField("vaultTokenRequester");
            tokenRequesterField.setAccessible(true);
            tokenRequesterField.set(vaultRestService, vaultTokenRequester);
            
            java.lang.reflect.Field tokenField = VaultRestService.class.getDeclaredField("vaultToken");
            tokenField.setAccessible(true);
            tokenField.set(vaultRestService, vaultToken);
        } catch (Exception e) {
            throw new RuntimeException("Failed to set up test", e);
        }
        
        // Setup token behavior
        when(vaultToken.getToken()).thenReturn("mock-token");
        doNothing().when(vaultToken).renewToken();
    }

    @Test
    public void testGetResponse_Success() {
        // Setup mock response
        String responseBody = "{ \"status\": \"success\", \"data\": \"test data\" }";
        server.expect(requestTo("http://api.example.com/test"))
                .andExpect(method(org.springframework.http.HttpMethod.GET))
                .andExpect(header("authorization", "Bearer mock-token"))
                .andRespond(withSuccess(responseBody, MediaType.APPLICATION_JSON));

        // Execute test
        Document result = vaultRestService.getResponse("http://api.example.com/test", "");
        
        // Verify
        assertThat(result).isNotNull();
        assertThat(result.getString("status")).isEqualTo("success");
        assertThat(result.getString("data")).isEqualTo("test data");
        
        // Verify token was renewed
        verify(vaultToken, times(1)).renewToken();
        
        // Verify server expectations
        server.verify();
    }

    // 5. Test validation with parameterized tests
    @ParameterizedTest
    @NullAndEmptySource
    @ValueSource(strings = {" ", "\t", "\n"})
    public void testGetResponse_InvalidUri(String uri) {
        // Execute test and expect exception
        assertThrows(IllegalArgumentException.class, () -> {
            vaultRestService.getResponse(uri, "test-data");
        });
    }

    // 5. Test validation with parameterized tests
    @ParameterizedTest
    @NullAndEmptySource
    @ValueSource(strings = {" ", "\t", "\n"})
    public void testGetErrorPositionInformation_InvalidJob(String job) {
        // Execute test and expect exception
        assertThrows(IllegalArgumentException.class, () -> {
            vaultRestService.getErrorPositionInformation(job, "riskId", "jobId", "cf");
        });
    }

    @Test
    public void testGetResponse_EmptyResponse() {
        // Setup mock response
        String responseBody = "{}";
        server.expect(requestTo("http://api.example.com/test"))
                .andExpect(method(org.springframework.http.HttpMethod.GET))
                .andExpect(header("authorization", "Bearer mock-token"))
                .andRespond(withSuccess(responseBody, MediaType.APPLICATION_JSON));

        // Execute test and expect exception
        SharinganException exception = assertThrows(SharinganException.class, () -> {
            vaultRestService.getResponse("http://api.example.com/test", "");
        });
        
        // Verify exception message
        assertThat(exception.getMessage()).contains("Empty or null response from service");
        
        // Verify server expectations
        server.verify();
    }

    @Test
    public void testGetResponse_ServerError() {
        // Setup mock response
        server.expect(requestTo("http://api.example.com/test"))
                .andExpect(method(org.springframework.http.HttpMethod.GET))
                .andExpect(header("authorization", "Bearer mock-token"))
                .andRespond(withStatus(HttpStatus.INTERNAL_SERVER_ERROR));

        // Execute test and expect exception
        SharinganException exception = assertThrows(SharinganException.class, () -> {
            vaultRestService.getResponse("http://api.example.com/test", "");
        });
        
        // Verify exception message
        assertThat(exception.getMessage()).contains("Calling vault REST service failed");
        
        // Verify server expectations
        server.verify();
    }

    @Test
    public void testGetErrorPositionInformation_Success() {
        // Setup expected URL with parameters
        String job = "job1";
        String riskSourceIds = "risk1,risk2";
        String jobUUID = "uuid123";
        String cf = "cf456";
        String expectedUrl = "http://api.example.com/error/job1/risk1,risk2/uuid123/cf456";
        
        // Setup mock response
        String responseBody = "{ \"position\": \"line 42\", \"error\": \"syntax error\" }";
        server.expect(requestTo(expectedUrl))
                .andExpect(method(org.springframework.http.HttpMethod.GET))
                .andExpect(header("authorization", "Bearer mock-token"))
                .andRespond(withSuccess(responseBody, MediaType.APPLICATION_JSON));

        // Execute test
        Document result = vaultRestService.getErrorPositionInformation(job, riskSourceIds, jobUUID, cf);
        
        // Verify
        assertThat(result).isNotNull();
        assertThat(result.getString("position")).isEqualTo("line 42");
        assertThat(result.getString("error")).isEqualTo("syntax error");
        
        // Verify server expectations
        server.verify();
    }

    @Test
    public void testGetParquetFileListings_Success() {
        // Setup expected URL with parameters
        String job = "job1";
        String jobUUID = "uuid123";
        String expectedUrl = "http://api.example.com/parquet/job1/uuid123";
        
        // Setup mock response
        String responseBody = "{ \"files\": [\"file1.parquet\", \"file2.parquet\"] }";
        server.expect(requestTo(expectedUrl))
                .andExpect(method(org.springframework.http.HttpMethod.GET))
                .andExpect(header("authorization", "Bearer mock-token"))
                .andRespond(withSuccess(responseBody, MediaType.APPLICATION_JSON));

        // Execute test
        Document result = vaultRestService.getParquetFileListings(job, jobUUID);
        
        // Verify
        assertThat(result).isNotNull();
        assertThat(result.get("files", java.util.List.class)).contains("file1.parquet", "file2.parquet");
        
        // Verify server expectations
        server.verify();
    }

    @Test
    public void testDownloadParquetFile_Success() {
        // Setup expected URL with parameters
        String bucketName = "bucket1";
        String objectName = "object1.parquet";
        String expectedUrl = "http://api.example.com/download/bucket1/object1.parquet";
        
        // Sample binary data
        byte[] sampleData = "binary parquet data".getBytes();
        
        // Setup mock response
        server.expect(requestTo(expectedUrl))
                .andExpect(method(org.springframework.http.HttpMethod.GET))
                .andExpect(header("authorization", "Bearer mock-token"))
                .andRespond(withSuccess(sampleData, MediaType.APPLICATION_OCTET_STREAM));

        // Execute test
        byte[] result = vaultRestService.downloadParquetFile(bucketName, objectName);
        
        // Verify
        assertThat(result).isEqualTo(sampleData);
        
        // Verify server expectations
        server.verify();
    }

    // 5. Test malformed response handling
    @Test
    public void testGetResponse_MalformedJsonResponse() {
        // Setup mock response with invalid JSON
        String invalidJson = "{ This is not valid JSON";
        server.expect(requestTo("http://api.example.com/test"))
                .andExpect(method(org.springframework.http.HttpMethod.GET))
                .andExpect(header("authorization", "Bearer mock-token"))
                .andRespond(withSuccess(invalidJson, MediaType.APPLICATION_JSON));

        // Execute test and expect exception
        SharinganException exception = assertThrows(SharinganException.class, () -> {
            vaultRestService.getResponse("http://api.example.com/test", "");
        });
        
        // Verify exception message
        assertThat(exception.getMessage()).contains("Calling vault REST service failed");
        
        // Verify server expectations
        server.verify();
    }
}


package com.example.service; // Assuming package name

import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.beans.factory.InitializingBean;
import org.springframework.http.HttpHeaders;
import org.springframework.http.MediaType;
import org.springframework.lang.NonNull;
import org.springframework.retry.annotation.Backoff;
import org.springframework.retry.annotation.Recover;
import org.springframework.retry.annotation.Retryable;
import org.springframework.retry.support.RetrySynchronizationManager;
import org.springframework.stereotype.Service;
import org.springframework.util.Assert;
import org.springframework.util.StringUtils;
import org.springframework.web.client.HttpStatusCodeException;
import org.springframework.web.client.RestClient;
import org.bson.Document;

import java.lang.annotation.ElementType;
import java.lang.annotation.Retention;
import java.lang.annotation.RetentionPolicy;
import java.lang.annotation.Target;
import java.util.Collections;
import java.util.UUID;

import static java.util.Objects.requireNonNull;

@Service
@Slf4j
@RequiredArgsConstructor
public class VaultRestService implements InitializingBean {

    private final PropertiesConfig propertiesConfig;
    private final RestClient restClient;
    
    private VaultTokenRequester vaultTokenRequester;
    private VaultToken vaultToken;

    // 1. Custom annotation for retry configuration
    @Target({ElementType.METHOD})
    @Retention(RetentionPolicy.RUNTIME)
    @Retryable(
            retryFor = SharinganException.class, 
            maxAttemptsExpression = "${vault.service.api.retry}",
            backoff = @Backoff(
                    delayExpression = "#{${context.service.retry.pause.duration} * 1000}", 
                    multiplier = 1
            )
    )
    public @interface VaultRetryable {}

    @Override
    public void afterPropertiesSet() {
        vaultTokenRequester = new VaultTokenRequester(
                propertiesConfig.getOauthUrl(),
                propertiesConfig.getOauthClientId(),
                propertiesConfig.getOauthClientSecret(),
                propertiesConfig.getVaultServiceUser(),
                propertiesConfig.getVaultServicePassword()
        );
        vaultToken = new VaultToken(this.vaultTokenRequester);
    }

    @VaultRetryable
    public Document getResponse(@NonNull String uri, String data) {
        Assert.hasText(uri, "URI must not be empty");
        
        String requestId = generateRequestId();
        log.info("Request [{}]: Calling endpoint {} with data: {}", requestId, uri, data);
        
        int retryCount = requireNonNull(RetrySynchronizationManager.getContext()).getRetryCount();
        
        if (retryCount != 0) {
            log.info("Request [{}]: Retry attempt {} for url {}, params: {}", requestId, retryCount, uri, data);
        }
        
        try {
            String response = executeGetRequest(uri, String.class, requestId);
            
            // 4. Added detailed logging
            if (log.isDebugEnabled()) {
                log.debug("Request [{}]: Received response: {}", requestId, 
                        response != null ? response.substring(0, Math.min(response.length(), 1000)) + "..." : "null");
            }
            
            if (StringUtils.hasText(response)) {
                Document document = Document.parse(response);
                if (document != null && !document.isEmpty()) {
                    log.info("Request [{}]: Successfully processed response from {}", requestId, uri);
                    return document;
                }
            }
            
            log.warn("Request [{}]: Empty response for service {}. Params = {}", requestId, uri, data);
            throw new SharinganException("Empty or null response from service");
            
        } catch (HttpStatusCodeException e) {
            log.error("Request [{}]: Error code {} received when calling {}, data: {}.", 
                    requestId, e.getStatusCode(), uri, data, e);
            throw new SharinganException("Calling vault REST service failed with status code: " + e.getStatusCode());
        } catch (Exception e) {
            log.error("Request [{}]: Error while getting data from {}. Params = {}", requestId, uri, data, e);
            throw new SharinganException("Calling vault REST service failed", e);
        }
    }

    @VaultRetryable
    public Document getErrorPositionInformation(@NonNull String job, @NonNull String riskSourceIds, 
                                            @NonNull String jobUUID, @NonNull String cf) {
        // 5. Input validation
        Assert.hasText(job, "Job must not be empty");
        Assert.hasText(riskSourceIds, "Risk source IDs must not be empty");
        Assert.hasText(jobUUID, "Job UUID must not be empty");
        Assert.hasText(cf, "CF must not be empty");
        
        String requestId = generateRequestId();
        log.info("Request [{}]: Getting error position for job: {}, riskSourceIds: {}, jobUUID: {}, cf: {}", 
                requestId, job, riskSourceIds, jobUUID, cf);
                
        String url = buildErrorPositionUrl(job, riskSourceIds, jobUUID, cf);
        log.info("Request [{}]: Calling getErrorPositionInformation with URL: {}", requestId, url);
        
        String response = executeGetRequest(url, String.class, requestId);
        
        // 4. Log response for debugging
        if (log.isDebugEnabled()) {
            log.debug("Request [{}]: Error position response: {}", requestId, 
                    response != null ? response.substring(0, Math.min(response.length(), 1000)) + "..." : "null");
        }
        
        Document result = Document.parse(response);
        log.info("Request [{}]: Successfully retrieved error position information", requestId);
        return result;
    }

    @VaultRetryable
    public Document getParquetFileListings(@NonNull String job, @NonNull String jobUUID) {
        // 5. Input validation
        Assert.hasText(job, "Job must not be empty");
        Assert.hasText(jobUUID, "Job UUID must not be empty");
        
        String requestId = generateRequestId();
        log.info("Request [{}]: Getting parquet file listings for job: {}, jobUUID: {}", 
                requestId, job, jobUUID);
                
        String url = buildParquetFileListingsUrl(job, jobUUID);
        log.info("Request [{}]: Calling getParquetFileListings with URL: {}", requestId, url);
        
        String response = executeGetRequest(url, String.class, requestId);
        
        // 4. Log response for debugging
        if (log.isDebugEnabled()) {
            log.debug("Request [{}]: Parquet file listings response: {}", requestId, 
                    response != null ? response.substring(0, Math.min(response.length(), 1000)) + "..." : "null");
        }
        
        Document result = Document.parse(response);
        log.info("Request [{}]: Successfully retrieved parquet file listings", requestId);
        return result;
    }

    @VaultRetryable
    public byte[] downloadParquetFile(@NonNull String bucketName, @NonNull String objectName) {
        // 5. Input validation
        Assert.hasText(bucketName, "Bucket name must not be empty");
        Assert.hasText(objectName, "Object name must not be empty");
        
        String requestId = generateRequestId();
        log.info("Request [{}]: Downloading parquet file from bucket: {}, object: {}", 
                requestId, bucketName, objectName);
                
        String url = buildDownloadParquetFileUrl(bucketName, objectName);
        log.info("Request [{}]: Calling downloadParquetFile with URL: {}", requestId, url);
        
        byte[] data = executeGetRequest(url, byte[].class, requestId);
        
        // 4. Log response size for debugging
        if (data != null) {
            log.debug("Request [{}]: Downloaded {} bytes from parquet file", requestId, data.length);
        } else {
            log.warn("Request [{}]: Downloaded parquet file is null", requestId);
        }
        
        log.info("Request [{}]: Successfully downloaded parquet file", requestId);
        return data;
    }

    // 5. Helper methods for URL building
    private String buildErrorPositionUrl(String job, String riskSourceIds, String jobUUID, String cf) {
        return propertiesConfig.getErrorPositionUrl()
                .replace("{job}", job)
                .replace("{riskSourceIds}", riskSourceIds)
                .replace("{jobUUID}", jobUUID)
                .replace("{cf}", cf);
    }
    
    private String buildParquetFileListingsUrl(String job, String jobUUID) {
        return propertiesConfig.getParquetFileListingsUrl()
                .replace("{job}", job)
                .replace("{jobUUID}", jobUUID);
    }
    
    private String buildDownloadParquetFileUrl(String bucketName, String objectName) {
        return propertiesConfig.getDownloadParquetFileUrl()
                .replace("{bucketName}", bucketName)
                .replace("{objectName}", objectName);
    }

    // 4. Added requestId for improved logging
    private <T> T executeGetRequest(String url, Class<T> responseType, String requestId) {
        try {
            log.debug("Request [{}]: Renewing token before request", requestId);
            vaultToken.renewToken();
            
            HttpHeaders headers = createHeaders();
            
            log.debug("Request [{}]: Executing GET request to {}", requestId, url);
            T response = restClient.get()
                    .uri(url)
                    .headers(httpHeaders -> httpHeaders.addAll(headers))
                    .retrieve()
                    .body(responseType);
                    
            log.debug("Request [{}]: Received response from {}", requestId, url);
            return response;
            
        } catch (HttpStatusCodeException e) {
            log.error("Request [{}]: Error code {} received when calling {}.", 
                    requestId, e.getStatusCode(), url, e);
            throw new SharinganException("Calling vault REST service failed", e);
        } catch (Exception e) {
            log.error("Request [{}]: Error while getting data from {}.", requestId, url, e);
            throw new SharinganException("Calling vault REST service failed", e);
        }
    }
    
    // 5. Helper method for creating headers
    private HttpHeaders createHeaders() {
        HttpHeaders headers = new HttpHeaders();
        headers.setAccept(Collections.singletonList(MediaType.APPLICATION_JSON));
        headers.setContentType(MediaType.APPLICATION_JSON);
        headers.set("authorization", "Bearer " + vaultToken.getToken());
        return headers;
    }
    
    // 4. Helper method for generating request IDs
    private String generateRequestId() {
        return UUID.randomUUID().toString().substring(0, 8);
    }

    @Recover
    public Document recoverDocument(SharinganException e, String uri, String data) {
        log.error("Failed to call url {} with params {} after {} retries.", 
                uri, data, propertiesConfig.getVaultServiceApiRetry());
        return null;
    }

    @Recover
    public Document recoverErrorPosition(SharinganException e, String job, String riskSourceIds, String jobUUID, String cf) {
        log.error("Failed to get error position information after {} retries. Job: {}, riskSourceIds: {}, jobUUID: {}, cf: {}", 
                propertiesConfig.getVaultServiceApiRetry(), job, riskSourceIds, jobUUID, cf);
        return null;
    }

    @Recover
    public Document recoverParquetListings(SharinganException e, String job, String jobUUID) {
        log.error("Failed to get parquet file listings after {} retries. Job: {}, jobUUID: {}", 
                propertiesConfig.getVaultServiceApiRetry(), job, jobUUID);
        return null;
    }

    @Recover
    public byte[] recoverDownload(SharinganException e, String bucketName, String objectName) {
        log.error("Failed to download parquet file after {} retries. bucketName: {}, objectName: {}", 
                propertiesConfig.getVaultServiceApiRetry(), bucketName, objectName);
        return null;
    }
}






@ExtendWith(MockitoExtension.class)
class YourTestClass {
    @Mock
    private RestClient restClient;

    @Mock
    private RestClient.RequestHeadersUriSpec<?> mockRequestHeadersUriSpec;

    @Mock
    private RestClient.RequestHeadersSpec<?> mockRequestHeadersSpec;

    @Mock
    private RestClient.ResponseSpec mockResponseSpec;

    @Mock
    private RestClient.ResponseBodySpec mockResponseBodySpec;

    @Test
    void downloadParquetFile() {
        // Arrange
        String bucketName = "bucket1";
        String objectName = "object1";
        String url = "http://localhost:8080/downloadParquetFile";
        byte[] fileContent = "file content".getBytes();

        // Mocking the entire method chain
        when(restClient.get()).thenReturn(mockRequestHeadersUriSpec);
        
        when(mockRequestHeadersUriSpec.uri(anyString()))
            .thenReturn(mockRequestHeadersSpec);
        
        when(mockRequestHeadersSpec.headers(any()))
            .thenReturn(mockRequestHeadersSpec);
        
        when(mockRequestHeadersSpec.retrieve())
            .thenReturn(mockResponseSpec);
        
        when(mockResponseSpec.body(byte[].class))
            .thenReturn(fileContent);

        // Act
        byte[] response = vaultRestService.downloadParquetFile(bucketName, objectName);

        // Assert
        assertArrayEquals(fileContent, response);
    }
}

#!/bin/bash
set -e  # Exit immediately if a command exits with a non-zero status

# ---------------------------
# Configuration Variables
# ---------------------------
PROJECT="project-prod"               # Your production project ID
DATASET="shared_usage_data"          # The dataset to hold the view
VIEW_NAME="prod_jobs_vw"             # Name of the view
REGION="US"                          # Region for the dataset (adjust if needed)

# ---------------------------
# Create the dataset if it doesn't already exist
# ---------------------------
echo "Ensuring dataset ${PROJECT}:${DATASET} exists..."
if ! bq --location=${REGION} ls ${PROJECT}:${DATASET} > /dev/null 2>&1; then
  bq --location=${REGION} mk --dataset ${PROJECT}:${DATASET}
  echo "Dataset created."
else
  echo "Dataset already exists."
fi

# ---------------------------
# Define the view query
# ---------------------------
# This query pulls usage data from the INFORMATION_SCHEMA for the last 90 days.
# Adjust the query as needed.
VIEW_QUERY="
SELECT
  project_id,
  user_email,
  creation_time,
  total_bytes_processed,
  total_slot_ms,
  query,
  (total_bytes_processed / POWER(2, 40)) * 5 AS estimated_cost_usd
FROM \`${PROJECT}.region-us.INFORMATION_SCHEMA.JOBS_BY_PROJECT\`
WHERE creation_time >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 90 DAY)
"

# ---------------------------
# Create (or update) the view
# ---------------------------
echo "Creating or updating view ${PROJECT}:${DATASET}.${VIEW_NAME}..."
bq mk --use_legacy_sql=false --view "${VIEW_QUERY}" ${PROJECT}:${DATASET}.${VIEW_NAME}
echo "View ${VIEW_NAME} created successfully in dataset ${DATASET}."

# ---------------------------
# End of script
# ---------------------------
CREATE OR REPLACE VIEW `project-prod.shared_usage_data.prod_jobs_vw` AS
SELECT
  project_id,
  user_email,
  creation_time,
  total_bytes_processed,
  total_slot_ms,
  query,
  (total_bytes_processed / POWER(2, 40)) * 5 AS estimated_cost_usd
FROM `project-prod.region-us.INFORMATION_SCHEMA.JOBS_BY_PROJECT`
WHERE creation_time >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 90 DAY);

Here is a brief overview of our discussion. Ramzi presented the rising BigQuery costs observed on Riskstore, illustrated by a new Looker Studio report. It became clear that most of these expenses result from the thousands of queries generated each day by the Looker Studio “recon” report.

We agreed that the real solution lies in optimizing these queries rather than simply limiting usage. Chris has already asked users to stop using the recon report temporarily, giving us time to investigate and improve. As the report owner, William will handle immediate fixes to reduce costs.

In the longer term, Chris and Richard suggested a Report Server API that would provide a data frame to users, allowing them to adapt the information as needed. Ramzi will collaborate with William to optimize and analyze the report further. Alexandre also recommended contacting Google for expert advice on optimizing BigQuery queries within Looker Studio

ITH base_jobs AS (
  SELECT
    project_id,
    user_email,
    creation_time,
    total_bytes_processed,
    total_slot_ms,
    query,
    -- Convert bytes to TB and calculate approximate cost ($5 per TB)
    (total_bytes_processed / POWER(2, 40)) * 5 as estimated_cost_usd
  FROM `project-dev.region-us.INFORMATION_SCHEMA.JOBS_BY_PROJECT`
  WHERE creation_time >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 90 DAY)
  UNION ALL
  SELECT 
    project_id,
    user_email,
    creation_time,
    total_bytes_processed,
    total_slot_ms,
    query,
    (total_bytes_processed / POWER(2, 40)) * 5 as estimated_cost_usd
  FROM `project-uat.region-us.INFORMATION_SCHEMA.JOBS_BY_PROJECT`
  WHERE creation_time >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 90 DAY)
),

daily_metrics AS (
  SELECT
    project_id,
    DATE(creation_time) as usage_date,
    COUNT(*) as query_count,
    SUM(total_bytes_processed) as total_bytes,
    SUM(total_slot_ms) as total_slot_ms,
    SUM(estimated_cost_usd) as daily_cost
  FROM base_jobs
  GROUP BY 1, 2
),

user_metrics AS (
  SELECT
    project_id,
    user_email,
    DATE(creation_time) as usage_date,
    COUNT(*) as query_count,
    SUM(total_bytes_processed) as total_bytes,
    SUM(estimated_cost_usd) as user_daily_cost,
    ARRAY_AGG(STRUCT(
      query,
      total_bytes_processed,
      estimated_cost_usd,
      creation_time
    ) ORDER BY estimated_cost_usd DESC LIMIT 10) as top_queries
  FROM base_jobs
  GROUP BY 1, 2, 3
)

SELECT
  -- Main metrics table for Looker Studio
  um.project_id,
  um.usage_date,
  um.user_email,
  um.query_count,
  um.total_bytes,
  um.user_daily_cost,
  dm.daily_cost as project_daily_cost,
  um.top_queries
FROM user_metrics um
JOIN daily_metrics dm 
  ON dm.project_id = um.project_id 
  AND dm.usage_date = um.usage_date
ORDER BY um.usage_date DESC, um.user_daily_cost DESC

Subject: Notice of New Feature Implementation & Preliminary Cost Estimate

Hello [Project Manager Name],

I hope you’re doing well. I want to give you a heads-up about a new feature we need to implement, which involves changes to how we retrieve position error data from our service provider. Previously, we used a single API call to get all position error details. Now, the provider has split the workflow into multiple endpoints and parquet files, requiring us to do extra integration steps and data merging.


---

What’s Changing

1. Multiple Endpoints:

One endpoint returns only the list of positions that are in error.

Another endpoint gives us a list of parquet files for a particular job.

A final endpoint serves up those parquet files.



2. Parquet File Processing:

We will need to download, parse, and merge data from these parquet files to retrieve the full error details for each position.

Each parquet file can be around 50MB in size, so we have to handle memory usage and possibly cache or persist data.



3. New Logic & Data Aggregation:

We’ll need to fetch positions in error, retrieve the list of files, parse them, and then merge the resulting data with the initial list of positions.





---

Work Involved & Preliminary Effort Estimate

Requirements & Design (1–2 days)
Clarifying data formats, deciding on how we store and merge parquet data, and creating a brief technical design.

Integration of New Endpoints (1–2 days)
Writing Spring services/clients to call the new endpoints reliably, with proper error handling.

Parquet Parsing & Data Aggregation (2–4 days)
Choosing a parquet library, parsing 50MB files, deciding on whether to store data in-memory or use a cache.

Merging & Business Logic (1–2 days)
Mapping each position to its error details, handling edge cases or missing data.

Performance & Resilience (1–2 days)
Addressing concurrency, memory usage, and caching strategies to handle files of this size.

Testing & Deployment (1–2 days)
Integration tests, user acceptance, and final rollout.


Overall, we anticipate roughly 7–13 days of active development (about 2–3 weeks of calendar time if one developer is assigned) to design, implement, test, and deploy this feature. This estimate includes time for potential performance tuning and handling any edge cases.


---

Potential Costs

If we assume a typical daily rate for a developer (or the internal equivalent cost to the project), the cost would align with (7–13 days) × (Daily Rate). This cost can vary based on factors such as:

Additional overhead if more than one developer is involved.

Unforeseen complexities with large file handling or integration issues.

Changes in project scope or requirement clarifications.



---

Risks & Recommendations

Memory Usage: 50MB parquet files are not huge, but we should be mindful if multiple files are processed in parallel.

Caching Strategy: If these files will be accessed repeatedly, a caching layer (like Redis) or short-term storage might be necessary.

Timeline Flexibility: We recommend a 20–30% contingency for unexpected issues, especially if the service provider’s APIs or data format change again.



---

Next Steps

I’ll finalize the technical design and confirm any outstanding questions with the provider.

Please let me know if you have any questions, or if you’d like to schedule a brief walkthrough of the plan.

Once approved, we can allocate the necessary development resources and begin implementation.


Thank you for your attention to this matter, and I look forward to your feedback.


---

Best Regards,
[Your Name]
[Your Title / Role]
[Project / Team Name]
[Contact Information]




Subject: Update on New Error Data Retrieval Feature & Storage Considerations

Hello [Project Manager Name],

I hope you’re doing well. I’d like to update you on the new feature implementation regarding how we retrieve error details for our risk positions, and highlight some concerns about file storage as we’ll be handling multiple jobs daily.


---

1. Summary of the New Workflow

Multiple Endpoints:
Our provider now supplies separate endpoints:

1. One returns the list of positions in error.


2. Another provides a list of Parquet files for a given job.


3. A third allows us to download each file.



Parquet Files (~50 MB each):
We need to parse and merge data from these files to fully retrieve error details per position.

Frequent Jobs:
Because we process many jobs per day, we could be dealing with several 50 MB files regularly.



---

2. Storage & Disk Usage Considerations

If we store all these Parquet files on local disk for every job, we’ll risk running into significant storage overhead. We have a few strategies to manage this:

1. Ephemeral Storage:

Download, parse, extract the relevant data, and delete the raw Parquet files.

This prevents disk usage from accumulating but means we won’t keep the original files unless explicitly needed for audits.



2. External Object Storage (e.g., S3/Blob):

If we need raw files for long-term reference, we can offload them to a cloud service.

Reduces local disk usage, but adds some operational overhead.



3. Persist Only Relevant Data:

Parse the Parquet files and store only the needed information in a database.

Ensures quick lookups without re-parsing.

We delete the full files once processing is done.




We’ll choose the most efficient approach based on whether we need to reprocess old data frequently or store raw files for compliance. Our initial inclination is to process and discard, as long as the provider can re-supply files if we ever need them again.


---

3. Effort & Cost Estimates

Implementing this new workflow involves:

1. Requirements & Design (1–2 days)


2. Endpoint Integration (1–2 days)


3. Parquet Parsing & Aggregation (2–4 days)


4. Merging & Business Logic (1–2 days)


5. Performance, Resilience (1–2 days)


6. Testing & Deployment (1–2 days)



That brings us to roughly 7–13 days of development time, or about 2–3 weeks on the calendar if a single developer is focused on it. The final cost will depend on our standard daily rate multiplied by the total days, plus a 20–30% buffer for unforeseen complexities.


---

4. Next Steps & Approval

Confirm Storage Approach: Decide whether we adopt ephemeral storage, external object storage, or a database approach for the files.

Finalize Design: Once we confirm the storage strategy, I can finalize the technical plan and integration details.

Implementation & Testing: We’ll proceed with development, ensure performance testing, and handle any scalability concerns.


Please let me know if you have any questions or would like to discuss the storage options in more detail. Once we have your go-ahead on the approach, we can begin implementation.

Thank you for your time, and I look forward to your feedback.


---

Best Regards,
[Your Name]
[Your Role / Team]
[Your Contact Information]




SELECT
  project_id,
  FORMAT_TIMESTAMP('%Y-%m', creation_time) AS usage_month,
  user_email,
  ROUND(SUM(bytes_processed) / (1024 * 1024 * 1024), 2) AS gb_processed,
  ROUND(SUM(bytes_processed) / (1024 * 1024 * 1024 * 1024) * 6.25, 2) AS total_cost_usd
FROM
  `dev_project.region-eu.INFORMATION_SCHEMA.JOBS_BY_PROJECT`
WHERE
  job_type = "QUERY"
  AND state = "DONE"
  AND creation_time >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 3 MONTH)
GROUP BY
  project_id, usage_month, user_email

UNION ALL

SELECT
  project_id,
  FORMAT_TIMESTAMP('%Y-%m', creation_time) AS usage_month,
  user_email,
  ROUND(SUM(bytes_processed) / (1024 * 1024 * 1024), 2) AS gb_processed,
  ROUND(SUM(bytes_processed) / (1024 * 1024 * 1024 * 1024) * 6.25, 2) AS total_cost_usd
FROM
  `uat_project.region-eu.INFORMATION_SCHEMA.JOBS_BY_PROJECT`
WHERE
  job_type = "QUERY"
  AND state = "DONE"
  AND creation_time >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 3 MONTH)
GROUP BY
  project_id, usage_month, user_email;

"Listing directory before deletion:"
                                ls -la /opt/risktore/maven/repository/log4j/log4j || echo "Directory not found."

pipeline {
    agent none
    stages {
        stage('Cleanup Old Log4j on All Nodes') {
            steps {
                script {
                    def nodesWithLabel = []
                    // Find all nodes with the label
                    node('your-slave-label') {
                        nodesWithLabel = env.NODE_NAME
                    }
                    // Iterate over the nodes
                    for (nodeName in nodesWithLabel) {
                        node(nodeName) {
                            echo "Cleaning up on node: ${nodeName}"
                            sh '''
                                if [ -d /opt/risktore/maven/repository/log4j/log4j/1.12.12 ]; then
                                    rm -rf /opt/risktore/maven/repository/log4j/log4j/1.12.12
                                    echo "Old Log4j dependency deleted on ${NODE_NAME}."
                                else
                                    echo "Log4j dependency not found or already deleted on ${NODE_NAME}."
                                fi
                            '''
                        }
                    }
                }
            }
        }
    }
}



SELECT
  user_email,
  query,
  ROUND(bytes_processed / (1024 * 1024 * 1024), 2) AS gb_processed,
  ROUND((bytes_processed / (1024 * 1024 * 1024 * 1024)) * 6.25, 2) AS estimated_cost_usd,
  start_time,
  job_type
FROM
  `region-eu.INFORMATION_SCHEMA.JOBS_BY_PROJECT`
WHERE
  job_type = "QUERY"
  AND state = "DONE"
  AND creation_time BETWEEN TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 30 DAY) AND CURRENT_TIMESTAMP()
ORDER BY
  estimated_cost_usd DESC
LIMIT 100;


// Input variables
var rollDate = 20230101;  // Replace with your target roll date
var processIds = ["PROCESS123", "PROCESS456"];  // Replace with your process IDs

// Backup collections
var jobStatusEventsBackup = "JobStatusEvents_Backup";
var jobStatusBackup = "JobStatus_Backup";
var fileStatusBackup = "FileStatus_Backup";

// Step 1: Backup `JobStatusEvents`
print("Backing up JobStatusEvents...");
db.JobStatusEvents.aggregate([
    {
        $match: {
            rolldate: rollDate,
            processId: { $in: processIds }
        }
    },
    {
        $merge: {
            into: jobStatusEventsBackup,
            whenMatched: "merge", // Merge if already exists
            whenNotMatched: "insert" // Insert new documents
        }
    }
]);
print("Backup for JobStatusEvents completed.");

// Step 2: Backup `JobStatus`
print("Backing up JobStatus...");
db.JobStatus.aggregate([
    {
        $match: {
            rolldate: rollDate,
            processId: { $in: processIds }
        }
    },
    {
        $merge: {
            into: jobStatusBackup,
            whenMatched: "merge",
            whenNotMatched: "insert"
        }
    }
]);
print("Backup for JobStatus completed.");

// Step 3: Backup `FileStatus`
print("Backing up FileStatus...");
db.FileStatus.aggregate([
    {
        $match: {
            rolldate: rollDate,
            processId: { $in: processIds }
        }
    },
    {
        $merge: {
            into: fileStatusBackup,
            whenMatched: "merge",
            whenNotMatched: "insert"
        }
    }
]);
print("Backup for FileStatus completed.");

// Step 4: Delete from `JobStatusEvents`
print("Deleting from JobStatusEvents...");
var eventsResult = db.JobStatusEvents.deleteMany({
    rolldate: rollDate,
    processId: { $in: processIds }
});
print("Deleted records from JobStatusEvents: " + eventsResult.deletedCount);

// Step 5: Delete from `JobStatus`
print("Deleting from JobStatus...");
var jobResult = db.JobStatus.deleteMany({
    rolldate: rollDate,
    processId: { $in: processIds }
});
print("Deleted records from JobStatus: " + jobResult.deletedCount);

// Step 6: Delete from `FileStatus`
print("Deleting from FileStatus...");
var fileResult = db.FileStatus.deleteMany({
    rolldate: rollDate,
    processId: { $in: processIds }
});
print("Deleted records from FileStatus: " + fileResult.deletedCount);

print("Rollback script completed successfully. Backups are available for recovery if needed.");




// Recover `JobStatusEvents`
print("Recovering JobStatusEvents...");
db.JobStatusEvents_Backup.aggregate([
    {
        $merge: {
            into: "JobStatusEvents",
            whenMatched: "merge",
            whenNotMatched: "insert"
        }
    }
]);
print("Recovery for JobStatusEvents completed.");

// Recover `JobStatus`
print("Recovering JobStatus...");
db.JobStatus_Backup.aggregate([
    {
        $merge: {
            into: "JobStatus",
            whenMatched: "merge",
            whenNotMatched: "insert"
        }
    }
]);
print("Recovery for JobStatus completed.");

// Recover `FileStatus`
print("Recovering FileStatus...");
db.FileStatus_Backup.aggregate([
    {
        $merge: {
            into: "FileStatus",
            whenMatched: "merge",
            whenNotMatched: "insert"
        }
    }
]);
print("Recovery for FileStatus completed.");

type(s).

jobType (array of strings): Job type categories to filter by.





3. Logic:

Latest Version: Automatically retrieve only the latest version of each job, using a version or updatedAt field to identify the most recent.

Filtering: Limit filtering to the specified fields (rollDate date range, jobId, and jobType) for simplicity.

Pagination and Sorting:

Include limit and page query parameters for pagination.

Default sorting by updatedAt in descending order.




4. Swagger UI Integration:

Interactive Documentation: Document the API in Swagger, specifying all parameters and their usage.

Examples: Provide sample requests in Swagger to illustrate different filter combinations.

Parameter Descriptions: Clearly document the purpose of each parameter.



5. Response:

Format: JSON array of job objects.

Fields: Return all fields in each job document.

Error Handling: Implement error handling for invalid parameters or query formats.



6. Example GET Query:

Example: Retrieve active jobs of types "engineering" and "management" within a specific date range, limiting results to the latest version:

GET /jobs?startDate=2024-01-01&endDate=2024-12-31&jobId=123,456&jobType=engineering,management&limit=10&page=1





---

Acceptance Criteria:

1. The /jobs endpoint retrieves only the latest version of each job, based on the updatedAt or version field.


2. Filters for rollDate (using startDate and endDate), jobId (single or multiple), and jobType (array) are functional and documented in Swagger.


3. The API includes pagination (limit and page parameters) and default sorting by updatedAt in descending order.


4. Swagger UI is configured with parameter descriptions and usage examples.


5. Response is formatted in JSON, containing all job fields by default.


6. Error handling is implemented for invalid or missing parameters.




---

Priority:

High

Story Points:

5


---

This refined approach simplifies the filters and keeps the API focused on core needs, making it more efficient to implement and easier for clients to use. Let me know if there are any further adjustments needed!


<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Query Builder</title>
    
    <!-- jQuery CDN -->
    <script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>
    
    <!-- jQuery QueryBuilder CSS and JS -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/jQuery-QueryBuilder/dist/css/query-builder.default.min.css">
    <script src="https://cdn.jsdelivr.net/npm/jQuery-QueryBuilder/dist/js/query-builder.standalone.min.js"></script>

    <style>
        /* Custom CSS for layout */
        body {
            font-family: Arial, sans-serif;
            margin: 0;
            padding: 0;
            display: flex;
            flex-direction: column;
            height: 100vh;
        }
        #builder {
            flex: 1;
            padding: 20px;
            background-color: #f8f9fa;
        }
        #json-display {
            height: 200px;
            padding: 20px;
            background-color: #ffffff;
            border-top: 1px solid #ddd;
            position: relative;
        }
        #json-output {
            width: 100%;
            height: 150px;
            font-family: monospace;
            border: 1px solid #ccc;
            padding: 10px;
            box-sizing: border-box;
            resize: none;
        }
        #copy-button {
            position: absolute;
            right: 20px;
            bottom: 10px;
            padding: 8px 12px;
            background-color: #007bff;
            color: white;
            border: none;
            border-radius: 5px;
            cursor: pointer;
        }
    </style>
</head>
<body>

    <!-- Query Builder Section -->
    <div id="builder"></div>

    <!-- JSON Output and Copy Button Section -->
    <div id="json-display">
        <textarea id="json-output" readonly></textarea>
        <button id="copy-button">Copy to Clipboard</button>
    </div>

    <script>
        // Initialize the Query Builder with basic fields
        $('#builder').queryBuilder({
            filters: [
                {
                    id: 'jobIds',
                    label: 'Job IDs',
                    type: 'string',
                    input: 'text',
                    operators: ['equal', 'not_equal', 'in', 'not_in'],
                },
                {
                    id: 'field',
                    label: 'Field',
                    type: 'string',
                    input: 'select',
                    values: {
                        'Result_snap': 'Result_snap',
                        'Result_context': 'Result_context',
                        'Result_EPI': 'Result_EPI',
                        'Result_HMSBook': 'Result_HMSBook',
                        'Result_JobId': 'Result_JobId',
                        'Result_InstrumentId': 'Result_InstrumentId',
                        'Result_Errors': 'Result_Errors'
                    },
                    operators: ['equal', 'not_equal']
                },
                {
                    id: 'aggFunc',
                    label: 'Aggregation Function',
                    type: 'string',
                    input: 'select',
                    values: {
                        'sum': 'sum',
                        'avg': 'avg',
                        'min': 'min',
                        'max': 'max',
                        'count': 'count',
                        'any_value': 'any_value'
                    }
                },
                {
                    id: 'pivot',
                    label: 'Pivot',
                    type: 'string',
                    operators: ['equal', 'not_equal']
                }
            ]
        });

        // Update JSON output whenever the query is updated
        $('#builder').on('afterUpdateRuleValue afterUpdateRuleFilter afterDeleteRule afterCreateRule', function() {
            var rules = $('#builder').queryBuilder('getRules');
            if (!$.isEmptyObject(rules)) {
                $('#json-output').val(JSON.stringify(rules, null, 2));
            } else {
                $('#json-output').val('');
            }
        });

        // Copy JSON to Clipboard
        $('#copy-button').on('click', function() {
            const jsonOutput = document.getElementById('json-output');
            jsonOutput.select();
            document.execCommand('copy');
            alert('JSON copied to clipboard!');
        });
    </script>

</body>
</html>



openapi: 3.0.1
info:
  title: Warehouse API
  version: 1.0.0
paths:
  /warehouse:
    post:
      summary: Submit a warehouse request
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/WarehouseRequest'
            examples:
              example1:
                summary: Example Request
                value:
                  jobIds:
                    - "2ab3909d-f423-4d61-aa94-30d8f0a6d918"
                  valueCols:
                    - field: "Result_snap"
                      aggFunc: "sum"
                    - field: "Result_context"
                      aggFunc: "avg"
                  pivot:
                    aggColumn: "Result_Exposure_Value"
                    pivotColumn: "Result_MultipriceStep"
                    pivotValues:
                      - "RollBasedPnL"
                      - "PortfolioPnL"
      responses:
        '200':
          description: Successful response
components:
  schemas:
    WarehouseRequest:
      type: object
      properties:
        jobIds:
          type: array
          items:
            type: string
          description: List of Job IDs
        valueCols:
          type: array
          items:
            $ref: '#/components/schemas/ColumnVO'
          description: List of value columns
        aggregation:
          type: array
          items:
            type: string
            enum:
              - Result_EPI
              - Result_Exposure_RiskFactor1
              - Result_Exposure_RiskFactor2
              - Result_Exposure_InstrumentId
          description: Aggregation columns
        pivot:
          $ref: '#/components/schemas/Pivot'
        filterModel:
          type: object
          additionalProperties:
            $ref: '#/components/schemas/ColumnFilter'
          description: Filter model for additional query filters
      required:
        - jobIds
        - valueCols
    ColumnVO:
      type: object
      properties:
        field:
          type: string
          enum:
            - Result_snap
            - Result_context
            - Result_EPI
            - Result_HMSBook
            - Result_JobId
            - Result_InstrumentId
            - Result_Errors
            - Result_PortfolioId
            - Result_LegalEntity
            - Result_Desk
          description: Field name
        aggFunc:
          type: string
          enum:
            - sum
            - avg
            - min
            - max
            - count
            - any_value
          description: Aggregation function (required if 'aggregation' exists)
    Pivot:
      type: object
      properties:
        aggColumn:
          type: string
          description: Aggregation column
        pivotColumn:
          type: string
          description: Pivot column
        pivotValues:
          type: array
          items:
            type: string
            enum:
              - RollBasedPnL
              - PortfolioPnL
              - HypoPnL
              - UnexplainedHypo
              - ThetaPnL
              - FxSpotPnL
          description: Pivot values
      required:
        - aggColumn
        - pivotColumn
        - pivotValues
    ColumnFilter:
      type: object
      properties:
        filterType:
          type: string
          enum:
            - set
            - range
            - date
            - text
          description: Type of filter to apply
        values:
          type: array
          items:
            type: string
          description: Values to filter by (for set-type filters)
        range:
          type: object
          properties:
            start:
              type: string
              format: date-time
              description: Start of the range (for range-type filters)
            end:
              type: string
              format: date-time
              description: End of the range (for range-type filters)
        text:
          type: string
          description: Text-based filter input
      description: Column filter options for filtering query results


openapi: 3.0.1
info:
  title: Warehouse API
  version: 1.0.0
paths:
  /warehouse:
    post:
      summary: Submit a warehouse request
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/WarehouseRequest'
      responses:
        '200':
          description: Successful response
components:
  schemas:
    WarehouseRequest:
      type: object
      properties:
        jobIds:
          type: array
          items:
            type: string
          description: List of Job IDs
        valueCols:
          type: array
          items:
            $ref: '#/components/schemas/ColumnVO'
          description: List of value columns
        aggregation:
          type: array
          items:
            type: string
            enum:
              - Result_EPI
              - Result_Exposure_RiskFactor1
              - Result_Exposure_RiskFactor2
              - Result_Exposure_InstrumentId
          description: Aggregation columns
        filterModel:
          type: object
          additionalProperties:
            $ref: '#/components/schemas/ColumnFilter'
        pivot:
          $ref: '#/components/schemas/Pivot'
        distinct:
          type: boolean
          default: false
        exportFormat:
          type: string
          enum:
            - avro
            - json
            - csv
          default: avro
      required:
        - jobIds
        - valueCols
      allOf:
        # Conditional validation: If 'aggregation' is present and not empty, then 'aggFunc' in 'valueCols' is required
        - if:
            properties:
              aggregation:
                minItems: 1
          then:
            properties:
              valueCols:
                type: array
                items:
                  $ref: '#/components/schemas/ColumnVOWithAggFunc'
    ColumnVO:
      type: object
      properties:
        field:
          type: string
          description: Field name
          enum:
            - Result_snap
            - Result_context
            - Result_EPI
            - Result_HMSBook
            - Result_JobId
            - Result_InstrumentId
            - Result_Errors
            - Result_PortfolioId
            - Result_LegalEntity
            - Result_Desk
        displayName:
          type: string
          description: Display name
    ColumnVOWithAggFunc:
      allOf:
        - $ref: '#/components/schemas/ColumnVO'
        - type: object
          properties:
            aggFunc:
              type: string
              description: Aggregation function
              enum:
                - sum
                - avg
                - min
                - max
                - count
                - any_value
          required:
            - aggFunc
    Pivot:
      type: object
      properties:
        aggColumn:
          type: string
          description: Aggregation column
        pivotColumn:
          type: string
          description: Pivot column
        pivotValues:
          type: array
          items:
            type: string
            enum:
              - RollBasedPnL
              - PortfolioPnL
              - HypoPnL
              - UnexplainedHypo
              - ThetaPnL
              - FxSpotPnL
      required:
        - aggColumn
        - pivotColumn
        - pivotValues


Sure! Here are the detailed examples for parts 4, 5, and 6 of the client's documentation.

### 4. Building a Query in JSON

**Structure of the JSON Request**
- The JSON request structure is designed to build SQL queries dynamically. Below is the general structure:

```json
{
  "noPagination": false,
  "pageNumber": 1,
  "pageSize": 10,
  "rowGroupCols": [
    {"field": "tablePrefix.columnName", "displayName": "columnAlias", "aggFunc": "aggregationFunction"}
  ],
  "valueCols": [
    {"field": "tablePrefix.columnName", "displayName": "columnAlias", "aggFunc": "aggregationFunction"}
  ],
  "selectCols": ["tablePrefix.columnName"],
  "groupKeys": [],
  "pivotColumn": "tablePrefix.columnName",
  "pivot": {
    "aggColumn": "tablePrefix.columnName",
    "pivotColumn": "tablePrefix.columnName",
    "pivotValues": ["value1", "value2"]
  },
  "filterModel": {
    "tablePrefix.columnName": {"values": ["value1", "value2"], "filterType": "set"}
  },
  "sortModel": [
    {"colId": "tablePrefix.columnName", "sort": "asc"}
  ]
}
```

**Detailed Explanation of Each JSON Field**

- `noPagination`: Boolean to disable pagination.
- `pageNumber`: The page number for pagination.
- `pageSize`: The size of each page.
- `rowGroupCols`: List of columns to group by. Each item should have:
  - `field`: The column name prefixed with the table alias.
  - `displayName`: An alias for the column in the results.
  - `aggFunc`: The aggregation function to apply (e.g., `SUM`, `COUNT`).
- `valueCols`: List of columns to aggregate. Each item should have:
  - `field`: The column name prefixed with the table alias.
  - `displayName`: An alias for the column in the results.
  - `aggFunc`: The aggregation function to apply.
- `selectCols`: List of columns to select.
- `groupKeys`: List of group keys.
- `pivotColumn`: Column to pivot on.
- `pivot`: An object containing:
  - `aggColumn`: Column to aggregate in the pivot.
  - `pivotColumn`: Column to pivot on.
  - `pivotValues`: List of values to pivot.
- `filterModel`: A map of filters. Each filter should have:
  - `values`: List of values to filter.
  - `filterType`: The type of filter (`set`, `number`, etc.).
- `sortModel`: List of sorting instructions. Each item should have:
  - `colId`: The column to sort by.
  - `sort`: The sort direction (`asc`, `desc`).

**Example JSON Queries**

1. **Basic Query**

```json
{
  "noPagination": false,
  "pageNumber": 1,
  "pageSize": 10,
  "rowGroupCols": [
    {"field": "cust.customerId", "displayName": "customerId", "aggFunc": null}
  ],
  "valueCols": [
    {"field": "ord.totalAmount", "displayName": "totalAmount", "aggFunc": "SUM"}
  ],
  "selectCols": ["cust.customerId", "ord.totalAmount"],
  "groupKeys": [],
  "filterModel": {
    "cust.customerId": {"values": ["123", "456"], "filterType": "set"}
  },
  "sortModel": [
    {"colId": "cust.customerId", "sort": "asc"}
  ]
}
```

2. **Pivot Query**

```json
{
  "noPagination": false,
  "pageNumber": 1,
  "pageSize": 10,
  "rowGroupCols": [
    {"field": "cust.customerId", "displayName": "customerId", "aggFunc": null},
    {"field": "ord.orderId", "displayName": "orderId", "aggFunc": null}
  ],
  "valueCols": [
    {"field": "ord.totalAmount", "displayName": "totalAmount", "aggFunc": "SUM"}
  ],
  "selectCols": ["cust.customerId", "ord.orderId", "ord.totalAmount"],
  "groupKeys": [],
  "pivotColumn": "ord.orderStatus",
  "pivot": {
    "aggColumn": "ord.totalAmount",
    "pivotColumn": "ord.orderStatus",
    "pivotValues": ["completed", "pending"]
  },
  "filterModel": {
    "cust.customerId": {"values": ["123", "456"], "filterType": "set"},
    "ord.totalAmount": {"filter": 100, "type": "greaterThan"}
  },
  "sortModel": [
    {"colId": "cust.customerId", "sort": "asc"}
  ]
}
```

### 5. API Endpoints

**List of Available Endpoints**

- **POST /api/v1/query**
  - Description: Generates a SQL query based on the JSON input.
  - Request Body: JSON object with query parameters.
  - Response: JSON object with the generated SQL query.

**Detailed Explanation of Each Endpoint**

**POST /api/v1/query**

- **URL**: `/api/v1/query`
- **HTTP Method**: `POST`
- **Description**: This endpoint generates a SQL query based on the JSON input provided.
- **Request Body**: The JSON request body should follow the structure described in the "Building a Query in JSON" section.
- **Response**:
  - `query`: The generated SQL query.

**Example API Requests and Responses**

**Request:**

```http
POST /api/v1/query
Content-Type: application/json

{
  "noPagination": false,
  "pageNumber": 1,
  "pageSize": 10,
  "rowGroupCols": [
    {"field": "cust.customerId", "displayName": "customerId", "aggFunc": null},
    {"field": "ord.orderId", "displayName": "orderId", "aggFunc": null}
  ],
  "valueCols": [
    {"field": "ord.totalAmount", "displayName": "totalAmount", "aggFunc": "SUM"}
  ],
  "selectCols": ["cust.customerId", "ord.orderId", "ord.totalAmount"],
  "groupKeys": [],
  "pivotColumn": "ord.orderStatus",
  "pivot": {
    "aggColumn": "ord.totalAmount",
    "pivotColumn": "ord.orderStatus",
    "pivotValues": ["completed", "pending"]
  },
  "filterModel": {
    "cust.customerId": {"values": ["123", "456"], "filterType": "set"},
    "ord.totalAmount": {"filter": 100, "type": "greaterThan"}
  },
  "sortModel": [
    {"colId": "cust.customerId", "sort": "asc"}
  ]
}
```

**Response:**

```json
{
  "query": "SELECT cust.customerId, ord.orderId, SUM(ord.totalAmount) as totalAmount FROM Customers AS cust LEFT JOIN UNNEST(cust.orders) AS ord WHERE cust.customerId IN ('123', '456') AND ord.totalAmount > 100 GROUP BY cust.customerId, ord.orderId ORDER BY cust.customerId asc PIVOT (SUM(ord.totalAmount) FOR ord.orderStatus IN ('completed', 'pending'))"
}
```

### 6. Common Use Cases

**Examples of Common Queries**

1. **Filtering Data**
   - Filter customers with specific IDs and sum their order amounts.

**Example JSON:**
```json
{
  "noPagination": false,
  "pageNumber": 1,
  "pageSize": 10,
  "rowGroupCols": [
    {"field": "cust.customerId", "displayName": "customerId", "aggFunc": null}
  ],
  "valueCols": [
    {"field": "ord.totalAmount", "displayName": "totalAmount", "aggFunc": "SUM"}
  ],
  "selectCols": ["cust.customerId", "ord.totalAmount"],
  "groupKeys": [],
  "filterModel": {
    "cust.customerId": {"values": ["123", "456"], "filterType": "set"}
  },
  "sortModel": [
    {"colId": "cust.customerId", "sort": "asc"}
  ]
}
```

**Example SQL:**
```sql
SELECT cust.customerId, SUM(ord.totalAmount) as totalAmount
FROM Customers AS cust
LEFT JOIN UNNEST(cust.orders) AS ord
WHERE cust.customerId IN ('123', '456')
GROUP BY cust.customerId
ORDER BY cust.customerId asc
```

2. **Grouping Data**
   - Group orders by customer and order ID, and sum the total amount.

**Example JSON:**
```json
{
  "noPagination": false,
  "pageNumber": 1,
  "pageSize": 10,
  "rowGroupCols": [
    {"field": "cust.customerId", "displayName": "customerId", "aggFunc": null},
    {"field": "ord.orderId", "displayName": "orderId", "aggFunc": null}
  ],
  "valueCols": [
    {"field": "ord.totalAmount", "displayName": "totalAmount", "aggFunc": "SUM"}
  ],
  "selectCols": ["cust.customerId", "ord.orderId", "ord.totalAmount"],
  "groupKeys
