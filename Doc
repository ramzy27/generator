Skip to content
 
Search Gists
Search...
All gists
Back to GitHub
@ramzy27
ramzy27/gist:22cbcb55d35b50443f24d8800a90ff71 Secret
Created 25 minutes ago
Code
Revisions
1
Clone this repository at &lt;script src=&quot;https://gist.github.com/ramzy27/22cbcb55d35b50443f24d8800a90ff71.js&quot;&gt;&lt;/script&gt;
<script src="https://gist.github.com/ramzy27/22cbcb55d35b50443f24d8800a90ff71.js"></script>
adrs
gistfile1.txt
# ADR-001 â€” Job Definitions Sourcing

## Status
Accepted

## Context
The Metadata service must provide the list of expected jobs (per snap, per business date) in a reliable and deterministic way.

Historically, job definitions were derived from Mongo data and implicitly tied to legacy systems (Sophis, Control-M) and later Airflow.

The new system requires:
- clear ownership of job definitions
- independence from runtime/orchestration systems
- support for business concepts such as snaps and period-based occurrences (daily, weekly, quarter, year-end)

## Options Considered

### Option 1 â€” Continue sourcing from Mongo (legacy continuation)
**Description**  
Reuse and extend existing Mongo-based job definitions.

**Pros**
- Low initial effort
- No new component introduced

**Cons**
- Inherits legacy constraints and ambiguity
- Poor versioning and auditability
- Implicit coupling with legacy systems
- Hard to evolve cleanly

---

### Option 2 â€” Correlate job definitions to Airflow
**Description**  
Derive job definitions from Airflow DAGs and metadata.

**Pros**
- Strong alignment with actual execution
- Single operational system

**Cons**
- Business metadata hard to express and maintain in DAG code
- Tight coupling to runtime/orchestration
- Not suitable for business-driven concepts (snaps, period ends)
- Limits independent evolution of the Metadata service

---

### Option 3 â€” Maintain a separate Job Registry (dissociated)
**Description**  
Create a dedicated job registry owned by the Metadata domain, independent of Airflow and legacy systems.

**Pros**
- Clear separation of responsibilities
- Explicit and stable business model
- Independent evolution
- Better governance and readability

**Cons**
- New registry to maintain
- Requires a distribution and versioning mechanism

## Decision
Adopt **Option 3**: maintain a **separate Job Registry**, fully dissociated from Airflow and legacy systems.

## Consequences
- Job definitions become an explicit Metadata responsibility
- Runtime systems (e.g. Airflow) are consumers, not sources of truth
- Additional effort is required to manage versioning and distribution
- Enables a clean, future-proof model for snaps and business occurrences


# ADR-002 â€” Job Definitions Storage and Distribution

## Status
Accepted

## Context
Job definitions must be:
- versioned and auditable
- safely promoted across GCP environments (dev / uat / prod)
- easy to review and rollback
- readable by the Metadata service at runtime

The organization already operates Firestore for metadata storage and GCS across environments.

## Options Considered

### Option 1 â€” Store job definitions in Firestore
**Description**  
Persist job definitions in Firestore and expose them through the Metadata API.

**Pros**
- Reuses existing infrastructure and APIs
- Allows dynamic updates without redeployment
- Potential UI-based editing

**Cons**
- Weak native versioning and diffing
- Changes harder to review
- Higher risk of configuration drift
- Rollback and promotion are operationally complex
- Less deterministic across environments

---

### Option 2 â€” Store job definitions in Git and deploy to GCS
**Description**  
Maintain job definitions in a dedicated Git repository, validate them in CI, and publish immutable releases to GCS.  
The Metadata service reads definitions from GCS.

**Pros**
- Strong versioning and auditability (Git)
- PR-based review and approval
- Deterministic releases
- Easy rollback
- Clear promotion flow across environments
- No runtime mutation of definitions

**Cons**
- Changes require a deployment cycle
- No direct UI-based editing

## Decision
Adopt **Option 2**: store job definitions in **Git** and distribute them via **immutable releases in GCS**, consumed by the Metadata service.

## Consequences
- Job definitions are managed as code
- All changes are reviewed, validated, and versioned
- Promotion across dev / uat / prod uses the same immutable artifacts
- Runtime behavior becomes deterministic and auditable
- Firestore remains available for runtime data or audit materialization, but not as the source of truth for definitions


# ADR-003 â€” Job Definition Data Model and Expected Jobs API

## Status
Accepted

## Context
The Metadata service must expose a deterministic list of jobs expected for a given business date and snap.  
Job definitions must support:
- multiple snaps per job
- business-based occurrences (daily, weekly, quarterly, year-end)
- snap-specific frequency and routing
- independence from runtime execution systems

A stable and explicit data model is required to avoid ambiguity and runtime interpretation.

## Options Considered

### Option 1 â€” Minimal model derived at runtime
**Description**  
Store minimal job information and compute snaps, frequency, and routing dynamically at runtime.

**Pros**
- Smaller static definitions
- High flexibility

**Cons**
- Hidden logic and implicit behavior
- Hard to audit and reproduce historical expectations
- Increased risk of inconsistent results
- Difficult to reason about expected jobs

---

### Option 2 â€” Explicit job definition model with snap targets
**Description**  
Define a canonical job model where each job explicitly declares:
- its snaps
- occurrence rules per snap
- expected runs
- routing metadata

**Pros**
- Clear and explicit business intent
- Deterministic evaluation
- Easy to audit and reason about
- Stable contract for consumers

**Cons**
- Slightly more verbose definitions
- Requires upfront schema design

---

## Decision
Adopt **Option 2**: define an explicit canonical job definition model with snap-specific targets and occurrence rules.

Expose a single API:
- `getExpectedJobs(businessDate, snaps[])`

## Consequences
- Job definitions become self-describing and deterministic
- Expected jobs can be reproduced for any historical date
- Runtime systems consume a stable and well-defined contract
- Additional upfront schema discipline is required


# ADR-004 â€” Promotion, Versioning, and Runtime Safeguards

## Status
Accepted

## Context
Job definitions must be safely promoted across GCP environments (dev, uat, prod) while ensuring:
- determinism at runtime
- safe rollback
- no partial or inconsistent updates
- traceability of which version was used on a given day

The Metadata service must not depend on mutable or implicit "latest" configurations.

## Options Considered

### Option 1 â€” Load latest available definitions at runtime
**Description**  
Always load the most recent job definitions from storage.

**Pros**
- Simple implementation
- Immediate availability of changes

**Cons**
- Non-deterministic behavior
- Race conditions between deployments and runtime calls
- Difficult rollback
- Hard to audit historical behavior

---

### Option 2 â€” Immutable releases with environment-specific pointers
**Description**  
Publish job definitions as immutable releases.  
Use an environment-specific pointer (channel) to select the active release.

**Pros**
- Deterministic runtime behavior
- Safe and explicit promotion
- Simple rollback by repointing
- Clear audit trail

**Cons**
- Slightly more complex deployment process
- Requires release and channel management

---

## Decision
Adopt **Option 2**: use immutable job definition releases with environment-specific channel pointers.

The Metadata service always reads the release referenced by its environment channel.

## Consequences
- Runtime behavior is deterministic and reproducible
- Rollback is fast and low-risk
- Promotion across environments uses identical artifacts
- The active catalog version is always explicit and auditable
@ramzy27
Comment
 
Leave a comment
 
Footer
Â© 2026 GitHub, Inc.
Footer navigation
Terms
Privacy
Security
Status
Community
Docs
Contact
Manage cookies
Do not share my personal information

import jakarta.servlet.http.HttpServletRequest;
import jakarta.servlet.http.HttpServletResponse;
import org.springframework.http.ResponseEntity;
import org.springframework.stereotype.Controller;
import org.springframework.web.bind.annotation.RequestMapping;

@Controller
public class AppErrorController implements org.springframework.boot.web.servlet.error.ErrorController {

  private static final String ATTR_STATUS = "jakarta.servlet.error.status_code";
  private static final String ATTR_URI    = "jakarta.servlet.error.request_uri";

  @RequestMapping("/error")
  public Object handleError(HttpServletRequest req) {
    Integer status = (Integer) req.getAttribute(ATTR_STATUS);
    String uri = (String) req.getAttribute(ATTR_URI);
    if (status == null) status = HttpServletResponse.SC_INTERNAL_SERVER_ERROR;
    if (uri == null) uri = "";

    // Only customize 404. Let others return a minimal body.
    if (status != 404) {
      return ResponseEntity.status(status).body("Error " + status);
    }

    // API or management paths â†’ JSON 404 (for scanners)
    if (isApiOrMgmt(uri)) {
      return ResponseEntity.status(404).body("Resource not found");
    }

    // File-like paths (have an extension) â†’ JSON 404 (donâ€™t leak index.html)
    if (hasFileExtension(uri)) {
      return ResponseEntity.status(404).body("Resource not found");
    }

    // Browser deep link (GET + Accept: text/html, not XHR) â†’ load SPA
    if (isHtmlNavigation(req)) {
      return "forward:/index.html";
    }

    // Fallback JSON 404
    return ResponseEntity.status(404).body("Resource not found");
  }

  private boolean isApiOrMgmt(String uri) {
    return uri.startsWith("/api")
        || uri.startsWith("/actuator")
        || uri.startsWith("/v3")
        || uri.startsWith("/swagger")
        || uri.startsWith("/webjars")
        || uri.startsWith("/assets")
        || uri.startsWith("/sophis-")
        || uri.startsWith("/vault-")
        || uri.startsWith("/viewer-status")
        || uri.startsWith("/enrich-status")
        || uri.startsWith("/scenario-jobs")
        || uri.startsWith("/multi-price-jobs")
        || uri.startsWith("/legacy-metrics");
  }

  private boolean hasFileExtension(String uri) {
    int slash = uri.lastIndexOf('/');
    int dot = uri.lastIndexOf('.');
    return dot > slash; // e.g., /a/b.js or /a.tar.gz
  }

  private boolean isHtmlNavigation(HttpServletRequest req) {
    if (!"GET".equalsIgnoreCase(req.getMethod())) return false;
    String accept = req.getHeader("Accept");
    String xhr = req.getHeader("X-Requested-With");
    boolean wantsHtml = accept != null && accept.contains("text/html");
    boolean notAjax = xhr == null || !"XMLHttpRequest".equalsIgnoreCase(xhr);
    return wantsHtml && notAjax;
  }
}

#!/bin/bash

# MongoDB mongosync version and platform
MONGOSYNC_VERSION="1.9.0"
PLATFORM="linux-x86_64"

# URL to download
DOWNLOAD_URL="https://downloads.mongodb.com/tools/mongosync/mongosync-${PLATFORM}-${MONGOSYNC_VERSION}.tgz"

# Create temp dir
mkdir -p /tmp/mongosync-install
cd /tmp/mongosync-install || exit 1

# Download
echo "ðŸ“¥ Downloading mongosync $MONGOSYNC_VERSION..."
curl -O "$DOWNLOAD_URL"

# Extract
echo "ðŸ“¦ Extracting..."
tar -xvzf mongosync-${PLATFORM}-${MONGOSYNC_VERSION}.tgz

# Move binary to /usr/local/bin
echo "ðŸš€ Installing mongosync to /usr/local/bin..."
sudo mv mongosync-${PLATFORM}-${MONGOSYNC_VERSION}/bin/mongosync /usr/local/bin/

# Check version
echo "âœ… Installed version:"
mongosync --version

# Cleanup
cd ~
rm -rf /tmp/mongosync-install

echo "ðŸŽ‰ mongosync $MONGOSYNC_VERSION installed successfully."


#!/usr/bin/env bash
# ---------- EDIT THESE -------------
export DB_UID="f116f93a-519c-208a-9a72-3ef6c9a1f081"   # from console
export LOCATION="nam5"                                 # your region
export DB_ID="riskstore-poc"
export DB_USER="poc_user"
export DB_PASS='Th1sIs$tr0ng!'                         # one-time pw

# If you sit behind a corporate proxy:
# export HTTPS_PROXY="http://proxy.mycorp.local:8080"
# export NO_PROXY="169.254.169.254,metadata.google.internal"

URI="mongodb://${DB_USER}:${DB_PASS}@${DB_UID}.${LOCATION}.firestore.goog:443/${DB_ID}?loadBalanced=true&tls=true&retryWrites=false&authMechanism=SCRAM-SHA-256"

echo "Testing interactive shell..."
mongosh "$URI" --eval 'db.runCommand({ping:1})'

echo "Importing a JSON file..."
mongoimport --uri "$URI" \
            --collection JobStats \
            --file jobstats.json \
            --numInsertionWorkers 32 \
            --maintainInsertionOrder=false

<table>
  <caption><strong>Firestore-Enterprise monthly cost estimate</strong></caption>
  <thead>
    <tr>
      <th>Scenario</th>
      <th>Billable storage<br>(GiB-month)</th>
      <th>Storage<br>@ $0.24 / GiB-mo</th>
      <th>Read units<br>/ month</th>
      <th>Reads<br>@ $0.05 / M</th>
      <th>Write units<br>/ month</th>
      <th>Writes<br>@ $0.26 / M</th>
      <th><u>Subtotal</u></th>
      <th>Docs / links</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><em>A&nbsp;â€“ keep every collection<br>(incl. RiskResults)</em></td>
      <td align="right">13 245</td>
      <td align="right">$ 3 179</td>
      <td align="right">2.53 B</td>
      <td align="right">$ 126</td>
      <td align="right">1.08 B</td>
      <td align="right">$ 282</td>
      <td align="right"><strong>$ 3 587</strong></td>
      <td><a href="https://cloud.google.com/products/firestore/mongodb-compatibility#pricing">price card</a></td>
    </tr>
    <tr>
      <td><em>B â€“ drop RiskResults<br>(â‰ˆ 6.7 TB logical)</em></td>
      <td align="right">5 353</td>
      <td align="right">$ 1 285</td>
      <td align="right">1.27 B</td>
      <td align="right">$ 63</td>
      <td align="right">0.54 B</td>
      <td align="right">$ 141</td>
      <td align="right"><strong>$ 1 489</strong></td>
      <td><a href="https://cloud.google.com/products/firestore/mongodb-compatibility#pricing">price card</a></td>
    </tr>
  </tbody>
  <tfoot>
    <tr>
      <td colspan="9">
        <small>
          * Totals exclude optional Point-in-Time Recovery (adds â‰ˆ 100 % to the storage line)  
          * Network egress inside the same region is free; inter-region starts at $0.01 / GB  
          * First 1 GiB storage + 50 k reads + 40 k writes per day are free
        </small>
      </td>
    </tr>
  </tfoot>
</table>


package com.yourcompany.exception;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.springframework.http.HttpStatus;
import org.springframework.http.ResponseEntity;
import org.springframework.web.HttpMediaTypeNotSupportedException;
import org.springframework.web.HttpRequestMethodNotSupportedException;
import org.springframework.web.bind.annotation.ControllerAdvice;
import org.springframework.web.bind.annotation.ExceptionHandler;
import org.springframework.web.servlet.NoHandlerFoundException;

import javax.servlet.http.HttpServletRequest;
import java.time.LocalDateTime;
import java.util.HashMap;
import java.util.Map;

@ControllerAdvice
public class GlobalExceptionHandler {

    private static final Logger logger = LoggerFactory.getLogger(GlobalExceptionHandler.class);

    @ExceptionHandler(HttpMediaTypeNotSupportedException.class)
    public ResponseEntity<Map<String, Object>> handleMediaTypeNotSupported(
            HttpMediaTypeNotSupportedException ex, HttpServletRequest request) {
        
        logger.warn("Media type not supported: {} for request: {}", ex.getContentType(), request.getRequestURI());
        
        Map<String, Object> response = createErrorResponse(
            HttpStatus.UNSUPPORTED_MEDIA_TYPE.value(),
            "Unsupported Media Type",
            "The media type '" + ex.getContentType() + "' is not supported",
            request.getRequestURI()
        );
        
        return new ResponseEntity<>(response, HttpStatus.UNSUPPORTED_MEDIA_TYPE);
    }

    @ExceptionHandler(HttpRequestMethodNotSupportedException.class)
    public ResponseEntity<Map<String, Object>> handleMethodNotSupported(
            HttpRequestMethodNotSupportedException ex, HttpServletRequest request) {
        
        logger.warn("Method not supported: {} for request: {}", ex.getMethod(), request.getRequestURI());
        
        Map<String, Object> response = createErrorResponse(
            HttpStatus.METHOD_NOT_ALLOWED.value(),
            "Method Not Allowed",
            "HTTP method '" + ex.getMethod() + "' is not supported for this endpoint",
            request.getRequestURI()
        );
        
        return new ResponseEntity<>(response, HttpStatus.METHOD_NOT_ALLOWED);
    }

    @ExceptionHandler(NoHandlerFoundException.class)
    public ResponseEntity<Map<String, Object>> handleNoHandlerFound(
            NoHandlerFoundException ex, HttpServletRequest request) {
        
        logger.warn("No handler found for: {} {}", ex.getHttpMethod(), ex.getRequestURL());
        
        Map<String, Object> response = createErrorResponse(
            HttpStatus.NOT_FOUND.value(),
            "Not Found",
            "The requested resource was not found",
            request.getRequestURI()
        );
        
        return new ResponseEntity<>(response, HttpStatus.NOT_FOUND);
    }

    @ExceptionHandler(IllegalArgumentException.class)
    public ResponseEntity<Map<String, Object>> handleIllegalArgument(
            IllegalArgumentException ex, HttpServletRequest request) {
        
        logger.warn("Invalid argument: {} for request: {}", ex.getMessage(), request.getRequestURI());
        
        Map<String, Object> response = createErrorResponse(
            HttpStatus.BAD_REQUEST.value(),
            "Bad Request",
            "Invalid request parameters",
            request.getRequestURI()
        );
        
        return new ResponseEntity<>(response, HttpStatus.BAD_REQUEST);
    }

    // Generic catch-all handler for any other exceptions
    @ExceptionHandler(Exception.class)
    public ResponseEntity<Map<String, Object>> handleGenericException(
            Exception ex, HttpServletRequest request) {
        
        logger.error("Unexpected error occurred: {} for request: {}", ex.getMessage(), request.getRequestURI(), ex);
        
        // Determine appropriate status code based on exception type
        HttpStatus status = determineStatusCode(ex);
        
        Map<String, Object> response = createErrorResponse(
            status.value(),
            status.getReasonPhrase(),
            "The request could not be processed",
            request.getRequestURI()
        );
        
        return new ResponseEntity<>(response, status);
    }
    
    private HttpStatus determineStatusCode(Exception ex) {
        // Security/authentication related exceptions
        if (ex.getClass().getName().contains("Security") || 
            ex.getClass().getName().contains("Authentication") ||
            ex.getClass().getName().contains("Access")) {
            return HttpStatus.UNAUTHORIZED;
        }
        
        // Validation/binding exceptions
        if (ex.getClass().getName().contains("Validation") ||
            ex.getClass().getName().contains("Binding") ||
            ex.getClass().getName().contains("MethodArgument")) {
            return HttpStatus.BAD_REQUEST;
        }
        
        // Database/persistence exceptions
        if (ex.getClass().getName().contains("DataAccess") ||
            ex.getClass().getName().contains("SQLException") ||
            ex.getClass().getName().contains("Persistence")) {
            return HttpStatus.INTERNAL_SERVER_ERROR;
        }
        
        // Timeout exceptions
        if (ex.getClass().getName().contains("Timeout") ||
            ex.getClass().getName().contains("SocketTimeout")) {
            return HttpStatus.REQUEST_TIMEOUT;
        }
        
        // For security scanning tools, return 400 for unknown exceptions
        // This prevents 500 errors while still being semantically reasonable
        return HttpStatus.BAD_REQUEST;
    }

    private Map<String, Object> createErrorResponse(int status, String error, String message, String path) {
        Map<String, Object> response = new HashMap<>();
        response.put("timestamp", LocalDateTime.now().toString());
        response.put("status", status);
        response.put("error", error);
        response.put("message", message);
        response.put("path", path);
        return response;
    }
}


@Controller
public class SpaController {
    
    @RequestMapping(value = "/**", method = RequestMethod.GET)
    public String forward(HttpServletRequest request) {
        String path = request.getRequestURI();
        
        // Don't forward these paths - let Spring handle them normally
        if (path.startsWith("/api/") || 
            path.startsWith("/actuator/") || 
            path.startsWith("/error") ||
            path.startsWith("/swagger-ui") ||
            path.startsWith("/v3/api-docs") ||
            path.startsWith("/swagger-resources") ||
            path.startsWith("/webjars/") ||
            path.contains(".")) { // Static files (js, css, images, etc.)
            return null; // Let Spring handle normally
        }
        
        return "forward:/index.html";
    }
}

@Configuration
public class WebConfig implements WebMvcConfigurer {
    
    @Override
    public void addViewControllers(ViewControllerRegistry registry) {
        // Handle SPA routes - forward to index.html for non-API, non-error paths
        registry.addViewController("/{spring:^(?!api|actuator|error|static)\\w+}")
                .setViewName("forward:/index.html");
        registry.addViewController("/**/{spring:^(?!api|actuator|error|static)\\w+}")
                .setViewName("forward:/index.html");
        registry.addViewController("/{spring:^(?!api|actuator|error|static)\\w+}/**{spring:?!(\\.js|\\.css|\\.png|\\.jpg|\\.jpeg|\\.gif|\\.ico|\\.svg|\\.woff|\\.woff2|\\.ttf)$}")
                .setViewName("forward:/index.html");
    }
}


@Configuration
public class TomcatConfig {

    @Bean
    public TomcatServletWebServerFactory servletContainer() {
        TomcatServletWebServerFactory tomcat = new TomcatServletWebServerFactory();
        
        tomcat.addEngineValves(new SecurityHeaderValve());
        
        return tomcat;
    }
    
    public static class SecurityHeaderValve extends ValveBase {
        
        @Override
        public void invoke(Request request, Response response) throws IOException, ServletException {
            // Add headers BEFORE processing
            response.setHeader("X-Content-Type-Options", "nosniff");
            response.setHeader("X-Frame-Options", "SAMEORIGIN");
            
            // Continue with the request
            getNext().invoke(request, response);
        }
    }
}

@Configuration
public class FilterConfig {

    @Bean
    public FilterRegistrationBean<SecurityHeaderFilter> securityHeaderFilter() {
        FilterRegistrationBean<SecurityHeaderFilter> registrationBean = new FilterRegistrationBean<>();
        registrationBean.setFilter(new SecurityHeaderFilter());
        registrationBean.addUrlPatterns("/*");
        registrationBean.setOrder(Ordered.HIGHEST_PRECEDENCE);
        return registrationBean;
    }
    
    public static class SecurityHeaderFilter implements Filter {
        @Override
        public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain)
                throws IOException, ServletException {
            
            HttpServletResponse httpResponse = (HttpServletResponse) response;
            httpResponse.setHeader("X-Content-Type-Options", "nosniff");
            
            chain.doFilter(request, response);
        }
    }
}
@Configuration
public class TomcatConfig {

    @Bean
    public TomcatServletWebServerFactory servletContainer() {
        TomcatServletWebServerFactory tomcat = new TomcatServletWebServerFactory();
        
        tomcat.addContextCustomizers(context -> {
            // Add HTTP Header Security Filter
            FilterDef filterDef = new FilterDef();
            filterDef.setFilterName("httpHeaderSecurity");
            filterDef.setFilterClass("org.apache.catalina.filters.HttpHeaderSecurityFilter");
            
            // Configure the filter parameters
            filterDef.addInitParameter("antiClickJackingOption", "SAMEORIGIN");
            filterDef.addInitParameter("blockContentTypeSniffing", "true");
            filterDef.addInitParameter("hstsEnabled", "false"); // Set to true if using HTTPS
            
            context.addFilterDef(filterDef);
            
            // Map the filter to all requests
            FilterMap filterMap = new FilterMap();
            filterMap.setFilterName("httpHeaderSecurity");
            filterMap.addURLPattern("/*");
            context.addFilterMap(filterMap);
        });
        
        return tomcat;
    }
}
<plugin>
  <groupId>org.glassfish.jaxb</groupId>
  <artifactId>jaxb-maven-plugin</artifactId>
  <version>3.0.2</version> <!-- Use 3.x for jakarta.* -->
  <executions>
    <execution>
      <id>xjc</id>
      <goals>
        <goal>xjc</goal>
      </goals>
      <configuration>
        <sources>
          <source>${project.basedir}/src/main/resources/xsd</source>
        </sources>
        <packageName>com.example.generated</packageName>
      </configuration>
    </execution>
  </executions>
</plugin>
public void parseRiskResultsToEnrichPriceError(JobNotifDoc jobNotifDoc, Document job, List<Document> riskResultDocuments) {
    try {
        log.info("Parsing Risk Results to Enrich Price Error");
        
        List<Document> pricingErrorPositions = new ArrayList<>();
        Map<Long, Integer> epiToValidPvCount = new HashMap<>();
        Integer partialErrorCount = 0;
        Integer fullErrorCount = 0;
        Set<Long> positionsSet = new HashSet<>();
        
        Document jobStatus = riskStoreRepository.findJobStatusByRolldateJobNameJobUUID(
            jobNotifDoc.getRolldate(), jobNotifDoc.getJobLabel(), jobNotifDoc.getJobUUID());
        
        String context = job.getString("context");
        String jobGroupRef = jobNotifDoc.getJobGroupRef();
        boolean isParquetJobGroup = propertiesConfig.getParquetJobGroups().contains(jobGroupRef);
        boolean isSesNmrf = "SES_NMRF".equalsIgnoreCase(jobGroupRef);
        boolean isRoleBasedPnl = jobGroupRef.contains(ROLE_BASED_PNL_EXPLAIN_CONTEXT);
        
        // Calculate expected number of scenarios once
        Integer expectedNumberScenarios = null;
        if (isParquetJobGroup) {
            expectedNumberScenarios = "SES_NMRF_MAX_LOSS".equalsIgnoreCase(jobStatus.getString("job")) ? 3 : 250;
        } else {
            expectedNumberScenarios = RiskStoreCommonUtil.getNestedInteger(jobStatus, "processes.scenarioload.expectedNumberScenarios");
            if (expectedNumberScenarios == null) {
                expectedNumberScenarios = VaRJobGroupScenarioAmount.fromString(jobGroupRef).getAmount();
            }
        }
        
        for (Document doc : riskResultDocuments) {
            Long riskSourceID = RiskStoreCommonUtil.getNestedLong(doc, "riskSourceID");
            positionsSet.add(riskSourceID);
            
            Document pep = new Document();
            pep.put("epi", riskSourceID);
            
            // Handle SES_NMRF specific logic
            if (isSesNmrf) {
                List<String> ids = RiskStoreCommonUtil.getNestedDocList(doc, "scenarioSets.modelSet.valuation.pv.tensor20.coordinates.ids");
                if (ids != null && !ids.isEmpty()) {
                    String[] parts = ids.get(0).split("/");
                    if (parts.length == 2) {
                        pep.put("starId", parts[0]);
                        pep.put("bucketId", parts[1]);
                    }
                }
            }
            
            // Collect all errors from different sources
            List<Document> errors = RiskStoreCommonUtil.getNestedDocList(doc, "scenarioSets.modelSet.valuation.pv.tensor20.coordinates.errors");
            if (errors == null || errors.isEmpty()) {
                errors = new ArrayList<>();
                List<Document> tensorList = RiskStoreCommonUtil.getNestedDocList(doc, "scenarioSets.modelSet.valuation.pv.tensor");
                if (tensorList != null) {
                    for (Document tensor : tensorList) {
                        List<Document> tensorErrors = (List<Document>) tensor.get("errors");
                        if (tensorErrors != null) {
                            errors.addAll(tensorErrors);
                        }
                    }
                }
            }
            
            if (errors == null) {
                continue;
            }
            
            // Determine if partial or full error
            boolean isPartialError = !JOB_CONTEXTS_ONLY_HAVE_FULL_PRICING_ERROR.contains(context) && 
                                   CollectionUtils.size(errors) < expectedNumberScenarios;
            
            if (isPartialError) {
                partialErrorCount++;
                pep.put("scenariosError", errors);
                pep.put("partial", true);
            } else {
                fullErrorCount++;
                pep.put("full", true);
                
                // Handle different error types for full errors
                if (isRoleBasedPnl) {
                    List<Document> explains = RiskStoreCommonUtil.getNestedDocList(doc, "scenarioSets.modelSet.PnL.explains");
                    List<String> explainErrors = new ArrayList<>();
                    if (explains != null) {
                        for (Document explain : explains) {
                            String error = RiskStoreCommonUtil.getNestedString(explain, "valuation.pv.error");
                            if (error != null) {
                                explainErrors.add(error);
                            }
                        }
                    }
                    pep.put("scenariosError", explainErrors);
                } else if (isParquetJobGroup) {
                    List<Map> err = RiskStoreCommonUtil.getNestedDocList(doc, "scenarioSets.modelSet.valuation.pv.tensor2D.coordinates.errors");
                    if (err != null && !err.isEmpty()) {
                        pep.put("error", err.get(0).get("error"));
                    }
                } else {
                    String pvPath = IMA_RTPL_EXPLAIN_CONTEXT.equals(context) ? 
                        "scenarioSets.modelSet.PnL.explains.valuation.pv" : 
                        "scenarioSets.modelSet.valuation.pv";
                    Document pv = RiskStoreCommonUtil.getNestedDoc(doc, pvPath);
                    if (pv != null) {
                        pep.put("error", RiskStoreCommonUtil.getNestedString(pv, "error"));
                    }
                }
            }
            
            // Handle valid PV count for non-parquet jobs
            if (!isParquetJobGroup) {
                List<Object> values = RiskStoreCommonUtil.getNestedObjectList(doc, "scenarioSets.modelSet.valuation.pv.tensor.values");
                if (values != null) {
                    getEPIValidPvCount(values, epiToValidPvCount, riskSourceID);
                }
            }
            
            pricingErrorPositions.add(pep);
        }
        
        updatePricingError(jobNotifDoc.getRolldate(), jobNotifDoc.getSnap(), jobNotifDoc.getJobUUID(), job, pricingErrorPositions);
        
        riskStoreRepository.updatePricingErrorDetails(
            jobNotifDoc.getRolldate(), jobNotifDoc.getSnap(), jobNotifDoc.getJobLabel(), 
            jobNotifDoc.getJobUUID(), partialErrorCount, fullErrorCount);
        
        log.info("Saving refdata for pricing error positions into RS mongo");
        riskStoreRepository.getAndSavePricingErrorDetails(
            jobNotifDoc.getRolldate(), jobNotifDoc.getSnap(), jobNotifDoc.getJobUUID(), 
            jobNotifDoc.getCutoff(), positionsSet, epiToValidPvCount);
            
    } catch (Exception e) {
        log.error("Failed to parse risk result to enrich price error.", e);
    }
}
