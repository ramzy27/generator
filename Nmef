

public Boolean getAndSavePositionControlPricingError(final Integer rolldate, final String snap, 
                                                    final String jobName, final String jobUUID, 
                                                    final String jobGroupRef, final String cutoff) throws Exception {
    
    Document job = riskStoreRepository.getJob(rolldate, snap, jobName);
    String snapUUID = RiskStoreCommonUtil.getSnapUUID(rolldate, snap, propertiesConfig.getVaultSnapSplitSuffix());
    
    // Check if job is parquetJobGroup
    boolean isParquetJobGroup = jobGroupRef != null && jobGroupRef.contains("parquet");
    
    if (isParquetJobGroup) {
        // Use the new API for parquet jobs
        return handleParquetJobGroup(rolldate, snap, jobName, jobUUID, snapUUID, job, cutoff);
    } else {
        // Use the existing implementation for non-parquet jobs
        return handleRegularJobGroup(rolldate, snap, jobName, jobUUID, jobGroupRef, snapUUID, job, cutoff);
    }
}

private Boolean handleParquetJobGroup(final Integer rolldate, final String snap, final String jobName, 
                                      final String jobUUID, final String snapUUID, Document job, 
                                      final String cutoff) throws Exception {
    List<Document> pricingErrorPositions = new ArrayList<>();
    Map<Long, Integer> epiToValidPvCount = new HashMap<>();
    Integer partialErrorCount = 0;
    Integer fullErrorCount = 0;
    Set<Long> positionsSet = new HashSet<>();
    
    try {
        // Call the new vault API
        Document response = vaultRestService.getControls(snapUUID, jobUUID, "pricingError");
        
        int resultCount = response != null && response.containsKey(RESP_RESULT_COUNT) ? 
                          response.getInteger(RESP_RESULT_COUNT) : 0;
        
        logger.info("Get response from vault for parquet job, the count of result: {}", resultCount);
        
        if (resultCount > 0) {
            List<Document> body = RiskStoreCommonUtil.getSubDocumentList(response, "body");
            
            for (Document doc : body) {
                // Extract position ID
                Long riskSourceID = RiskStoreCommonUtil.getNestedLong(doc, "riskSourceID");
                positionsSet.add(riskSourceID);
                
                // Create minimal document for position error
                Document pep = new Document();
                pep.put("riskSourceID", riskSourceID);
                pep.put("full", true);  // Assume full error for parquet jobs in first iteration
                fullErrorCount++;
                
                pricingErrorPositions.add(pep);
            }
            
            // Save the results as before
            updatePricingError(rolldate, snap, jobUUID, job, pricingErrorPositions);
            riskStoreRepository.updatePricingErrorDetails(rolldate, snap, jobName, jobUUID, 
                                                         partialErrorCount, fullErrorCount);
            
            // Save position details
            logger.info("saving refdata for pricing error positions into RS mongo for parquet job");
            riskStoreRepository.getAndSavePricingErrorDetails(rolldate, snap, jobUUID, cutoff, 
                                                            positionsSet, epiToValidPvCount);
        }
        
        return true;
    } catch (Exception e) {
        logger.error("cannot get Vault metadata for parquet job.", e);
        return false;
    }
}

private Boolean handleRegularJobGroup(final Integer rolldate, final String snap, final String jobName,
                                     final String jobUUID, final String jobGroupRef, final String snapUUID,
                                     Document job, final String cutoff) throws Exception {
    // This would contain the original implementation that was in getAndSavePositionControlPricingError
    List<Document> pricingErrorPositions = new ArrayList<>();
    Map<Long, Integer> epiToValidPvCount = new HashMap<>();
    Integer partialErrorCount = 0;
    Integer fullErrorCount = 0;
    
    String url = MessageFormat.format(propertiesConfig.getVaultServiceGetResult(), 
                                     snapUUID, jobUUID, 8, 720)
        + "&p=riskSourceID"
        + "&p=scenarioSets.modelSet.valuation.pv.tensor.values"
        + "&p=scenarioSets.modelSet.valuation.pv.tensor.errors"
        + "&p=scenarioSets.modelSet.valuation.pv.error"
        // IMA_RTPL_EXPLAIN structure
        + "&p=scenarioSets.modelSet.PnL.explains.valuation.pv.error"
        + "&cf=pricingError";
    
    logger.info("try to ask vault for rolldate: {}, snap:{} url: {}", rolldate, snapUUID, url);
    
    try {
        Document response = vaultRestService.getResponse(url, null);
        
        int resultCount = response != null && response.containsKey(RESP_RESULT_COUNT) ? 
                          response.getInteger(RESP_RESULT_COUNT) : 0;
        
        logger.info("get response from vault, the count of result: {}", resultCount);
        
        if (resultCount > 0) {
            List<Document> body = RiskStoreCommonUtil.getSubDocumentList(response, "body");
            Set<Long> positionsSet = new HashSet<>();
            Document jobStatus = riskStoreRepository.findJobStatusByRolldateJobNameJobUUID(rolldate, jobName, jobUUID);
            
            // Rest of original implementation...
            for (Document doc : body) {
                // Original processing logic here...
                Long riskSourceID = RiskStoreCommonUtil.getNestedLong(doc, "riskSourceID");
                positionsSet.add(riskSourceID);
                
                Document pep = new Document();
                pep.put("riskSourceID", riskSourceID);
                
                List<Document> errors = RiskStoreCommonUtil.getNestedDocList(doc, 
                                      "scenarioSets.modelSet.valuation.pv.tensor.errors");
                
                Integer expectedNumberScenarios = null;
                if (null != jobStatus) {
                    expectedNumberScenarios = RiskStoreCommonUtil.getNestedInteger(jobStatus, 
                                           "processes.scenarioLoad.expectedNumberScenarios");
                }
                
                String context = job.getString("context");
                
                // Process errors and other logic as in the original code
                // ...
                
                pricingErrorPositions.add(pep);
            }
            
            updatePricingError(rolldate, snap, jobUUID, job, pricingErrorPositions);
            riskStoreRepository.updatePricingErrorDetails(rolldate, snap, jobName, jobUUID, 
                                                         partialErrorCount, fullErrorCount);
            
            logger.info("saving refdata for pricing error positions into RS mongo");
            riskStoreRepository.getAndSavePricingErrorDetails(rolldate, snap, jobUUID, cutoff, 
                                                             positionsSet, epiToValidPvCount);
        }
        
        return true;
    } catch (Exception e) {
        logger.error("cannot get Vault metadata.", e);
        return false;
    }
}

import requests
import subprocess
import shutil
import os
from pathlib import Path
from getpass import getpass

BITBUCKET_BASE_URL = "https://your-bitbucket-server.com"  # Replace with your Bitbucket URL
PROJECT_KEY = "your-project-key"  # Replace with your Bitbucket project key

username = input("Bitbucket Username: ")
password = getpass("Bitbucket Password: ")

def get_repos():
    repos = []
    api_url = f"{BITBUCKET_BASE_URL}/rest/api/1.0/projects/{PROJECT_KEY}/repos"
    response = requests.get(api_url, auth=(username, password))
    response_json = response.json()

    for repo in response_json['values']:
        repos.append(repo['links']['clone'][0]['href'])

    return repos

def check_log4j(repo_url, username, password):
    temp_dir = Path("./temp_repo")
    if temp_dir.exists():
        shutil.rmtree(temp_dir)

    auth_repo_url = repo_url.replace("https://", f"https://{username}:{password}@")

    print(f"\nScanning {repo_url}")
    try:
        subprocess.run(["git", "clone", "--depth", "1", auth_repo_url, temp_dir],
                       stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL, check=True)

        pom_file = temp_dir / "pom.xml"
        if pom_file.exists():
            result = subprocess.run(
                ["mvn", "dependency:tree", "-Dverbose", "-Dincludes=*log4j*"],
                cwd=temp_dir,
                capture_output=True, text=True)

            if "log4j:log4j" in result.stdout:
                print(f"[!] Old log4j detected in {repo_url}")
                return True
            else:
                print(f"[OK] No old log4j in {repo_url}")
                return False
        else:
            print(f"[SKIP] pom.xml not found in {repo_url}")
            return False
    except subprocess.CalledProcessError as e:
        print(f"[ERROR] Could not scan {repo_url}: {e}")
        return False
    finally:
        shutil.rmtree(temp_dir)

if __name__ == "__main__":
    username = input("Enter your Bitbucket username: ")
    password = getpass("Enter your Bitbucket password: ")
    
    repos = get_repos()
    vulnerable_repos = []

    for repo in repos:
        auth_repo = repo.replace("https://", f"https://{username}:{password}@")
        if check_log4j(authenticated_repo_url, username, password):
            vulnerable_repos.append(repo)

    print("\n--- Summary ---")
    if vulnerable_repos:
        print("Repositories using old Log4j:")
        for repo in vulnerable_repos:
            print(f"- {repo}")
    else:
        print("No repositories with old Log4j detected.")

### Usage:

- Replace:
  - `BITBUCKET_BASE_URL` with your Bitbucket URL.
  - `PROJECT_KEY` with your actual project key.
- Run the script:
```bash
python check_log4j.py
pipeline {
    agent any

    stages {
        stage('Detect Old Log4j Dependencies') {
            steps {
                script {
                    // Adjust the base directory if needed
                    def baseDir = '.'
                    def log4jPattern = ~/log4j:log4j/

                    def projectsWithOldLog4j = []

                    sh """
                        find ${baseDir} -name 'pom.xml' | while read pom; do
                            projectDir=\$(dirname \${pom})
                            echo "Scanning \${projectDir}..."
                            output=\$(cd \${projectDir} && mvn dependency:tree -Dverbose -Dincludes=*log4j* -DoutputFile=log4j_deps.txt)

                            if grep -E "log4j:log4j" \${projectDir}/log4j_deps.txt > /dev/null; then
                                echo "Old Log4j found in \${projectDir}"
                                echo \${projectDir} >> projects_with_old_log4j.txt
                            fi

                            rm \${projectDir}/log4j_deps.txt
                        done
                    """

                    // Read and print projects with issues
                    if (fileExists('projects_with_old_log4j.txt')) {
                        projectsWithOldLog4j = readFile('projects_with_old_log4j.txt').trim().split('\n')
                        echo "\u001B[31mProjects with old log4j detected:\u001B[0m"
                        projectsWithOldLog4j.each { project ->
                            echo "- ${project}"
                        }

                        error("Old log4j dependencies detected! See above.")
                    } else {
                        echo "\u001B[32mNo old Log4j dependencies detected.\u001B[0m"
                    }
                }
            }
        }
    }

    post {
        always {
            cleanWs()
        }
    }
}


# ADR001 - Separate Endpoints for Query Execution and File Retrieval

## Status
**Accepted**

---

## Context

Our current service processes a user query through the following steps:
1. Generates SQL from the query model.
2. Executes the query in BigQuery.
3. Exports the results to a GCS bucket.
4. Streams the binary content of the results back to the user.

Initially, we had a single synchronous endpoint to handle this entire process. However, we encountered the following challenges:
- **Long Query Execution Times**: Some queries take too long to execute, causing the client to time out.
- **BigQuery Export Size Limitation**: BigQuery has a maximum export file size of 1GB. For results larger than 1GB, the export must be split into multiple files.
- **Complex Aggregation**: Streaming back aggregated results from multiple files to the user became complex and error-prone.

To address these challenges, we decided to separate the functionality into two distinct endpoints.

---

## Options Considered

### Option 1: Separate Endpoints for Query Execution and File Retrieval
- **Description**: Create two endpoints:
  1. An endpoint to launch the query, which returns metadata, including links to the generated result files in GCS.
  2. An endpoint to retrieve the content of a specific result file by its link or identifier.

- **Pros**:
  - Decouples long-running query execution from result streaming.
  - Simplifies the streaming process since each file can be fetched individually.
  - Supports BigQuery’s split-export functionality natively.
  - Allows better handling of large result sets and timeouts.
  - Improves scalability by enabling asynchronous processing.

- **Cons**:
  - Requires the client to make multiple requests (one to launch the query, and another to fetch results).
  - Additional complexity in tracking the status of the query and managing links to result files.

---

### Option 2: Keep the Single Synchronous Endpoint
- **Description**: Continue using a single synchronous endpoint that handles query execution, exporting, and streaming the results back to the user.

- **Pros**:
  - Simpler for the client since only one request is needed.
  - No need for the client to handle metadata or track result files.

- **Cons**:
  - Timeout issues with long-running queries.
  - Cannot handle results exceeding BigQuery’s 1GB export size.
  - Aggregating and streaming large results back to the user is complex and error-prone.
  - Limited scalability and prone to failures under heavy load.

---

## Decision

We decided to implement **Option 1: Separate Endpoints for Query Execution and File Retrieval**.  
This approach addresses the timeout and export size limitations, simplifies the result streaming process, and supports asynchronous workflows for large result sets.

---

## Consequences

1. **Implementation Changes**:
   - Create two endpoints:
     - **Query Execution Endpoint**: Accepts the query model and triggers the BigQuery job. Returns metadata with links to the exported result files.
     - **File Retrieval Endpoint**: Accepts a file link or identifier and streams the content of the specified file to the user.
   - Store metadata about the result files, including their links, in a persistent data store if needed.

2. **Client-Side Changes**:
   - Clients must first call the Query Execution Endpoint to get the metadata and then use the File Retrieval Endpoint to fetch the results.

3. **Monitoring and Error Handling**:
   - Implement robust monitoring for query execution and file exports.
   - Ensure retry mechanisms for file retrieval in case of transient errors.

4. **Performance and Scalability**:
   - Improves scalability by decoupling query execution from result streaming.
   - Handles large result sets efficiently with BigQuery’s split-export feature.

---

## See Also

- [BigQuery documentation on Exporting Data](https://cloud.google.com/bigquery/docs/exporting-data)
- [GCS documentation on Managing Objects](https://cloud.google.com/storage/docs/objects)
