### JIRA: Organize Swagger UI, Fix Tests, and Refactor Code

**Summary:** Organize controllers in sections in the Swagger UI, fix or update unit tests, and refactor code for better naming conventions and alignment.

**Description:**
- **Objective:** Improve the project's organization and maintainability by organizing Swagger UI, updating unit tests, and refactoring code.

**Tasks:**
1. **Organize Controllers in Swagger UI:**
   - Group related controllers into sections for better readability in Swagger UI.
   - Ensure each section is clearly labeled and accessible.

2. **Fix Unit Tests:**
   - Review all existing unit tests.
   - Fix any failing or outdated tests.
   - Delete unit tests that are no longer relevant or used.
   - Add new unit tests where necessary to ensure coverage and reliability.

3. **Refactor Code:**
   - Review and refactor code to ensure collection names align with their purpose.
   - Update any misleading or incorrect names in the codebase.
   - Ensure consistency in naming conventions throughout the project.

**Acceptance Criteria:**
- Controllers are organized into clear sections in the Swagger UI.
- All unit tests are reviewed, with outdated tests removed and new tests added as needed.
- Code is refactored to ensure all names are accurate and aligned with their purpose.
- The project is more maintainable and easier to navigate.

---

Feel free to copy and paste this JIRA description as needed. Let me know if you need any further adjustments or additional details.

### JIRA 1: Adapt Keycloak Configuration for LDAP Integration

**Summary:** Adapt Keycloak configuration to integrate LDAP filters, groups, and populate them in the token. Modify the code to read these groups from the token.

**Description:**
- **Objective:** Enhance Keycloak configuration to support LDAP filters and groups.
- **Tasks:**
  1. Configure Keycloak to use LDAP as the user federation provider.
  2. Set up LDAP filters to retrieve specific user attributes.
  3. Define new LDAP groups and ensure they are included in the token.
  4. Modify the application code to read and process these groups from the token.
  5. Test the integration to ensure that the tokens contain the correct LDAP groups and attributes.

**Acceptance Criteria:**
- LDAP filters are correctly configured in Keycloak.
- New LDAP groups are created and populated in the token.
- The application successfully reads and processes the groups from the token.

---

### JIRA 2: Adapt SQL Query to Include User-Authorized Desks

**Summary:** Adapt the SQL query to ensure it includes only the desks that the user is authorized to view.

**Description:**
- **Objective:** Modify the SQL query to filter results based on user authorization.
- **Tasks:**
  1. Identify the current SQL query used to retrieve desks.
  2. Determine the authorization criteria for users.
  3. Modify the SQL query to include a join or subquery that filters desks based on the user's authorization.
  4. Test the updated query to ensure it returns only the desks the user is allowed to view.
  5. Validate the changes with sample user scenarios to confirm the correctness of the filter.

**Acceptance Criteria:**
- The SQL query is updated to include user authorization checks.
- Only desks the user is authorized to view are returned by the query.
- The query is tested and validated with various user scenarios.

---

### JIRA 3: Enhance Cache Handling Based on User Group

**Summary:** Modify cache handling to include user group information before processing requests.

**Description:**
- **Objective:** Improve the caching mechanism to consider the user group in the cache key.
- **Tasks:**
  1. Analyze the current caching mechanism to understand how user requests are cached.
  2. Identify the process to determine the userâ€™s group before processing the request.
  3. Modify the caching logic to include the user group in the cache key/hash.
  4. Ensure that the user group is correctly determined and incorporated into the cache key before any processing.
  5. Test the updated caching mechanism to verify that requests are cached and retrieved based on user group.
  6. Validate that the changes improve cache accuracy and efficiency.

**Acceptance Criteria:**
- The caching mechanism is updated to include the user group in the cache key.
- User group information is correctly determined and used before processing requests.
- The cache correctly handles and retrieves data based on the user group.
- The updated cache mechanism is tested and validated for accuracy and efficiency.

---

### JIRA 4: Ensure Compatibility of User-Added Filters with Authorized Desk Filter

**Summary:** Ensure that user-specified filters (e.g., DESk, EPI) work correctly with the authorized desk filter without conflicts.

**Description:**
- **Objective:** Modify the filtering logic to integrate user-specified filters with the authorized desk filter seamlessly.
- **Tasks:**
  1. Review the current filtering mechanism that processes user-specified filters.
  2. Integrate the authorized desk filter into the filtering logic.
  3. Ensure that the authorized desk filter is applied first, followed by any additional user-specified filters.
  4. Modify the code to handle the combination of filters without conflicts.
  5. Test various scenarios where users apply different filters to ensure they work correctly with the authorized desk filter.
  6. Validate that the results respect both the user's authorized view and their additional filters.

**Acceptance Criteria:**
- The filtering logic correctly integrates the authorized desk filter with user-specified filters.
- There are no conflicts between the authorized desk filter and additional user filters.
- The query results are accurate and respect both the user's authorized view and their additional filters.
- The solution is tested and validated for different filter combinations to ensure correctness.

---

Feel free to copy and paste these JIRA descriptions as needed. Let me know if you need any further assistance.


List<RiskMeasureName> riskMeasureNames = Arrays.asList(RiskMeasureName.values());
List<RiskLabel> riskLabels = Arrays.asList(RiskLabel.values());

List<String> foundRiskMeasureNames = sqlRequest.getValueCols().stream()
    .map(columnVO -> columnVO.field())
    .flatMap(field -> Stream.concat(riskMeasureNames.stream(), riskLabels.stream())
        .filter(riskMeasureName -> field.contains(riskMeasureName.name()))
        .map(Enum::name))
    .distinct()
    .collect(Collectors.toList());

Set<String> riskNames = Stream.concat(
    Arrays.stream(RiskMeasureName.values()).map(Enum::name),
    Arrays.stream(RiskLabel.values()).map(Enum::name)
).collect(Collectors.toSet());

List<String> foundRiskMeasureNames = sqlRequest.getValueCols().stream()
    .map(columnVO -> columnVO.field())
    .flatMap(field -> riskNames.stream().filter(field::contains))
    .distinct()
    .collect(Collectors.toList());

Set<String> riskNames = Stream.concat(
    Arrays.stream(RiskMeasureName.values()).map(Enum::name),
    Arrays.stream(RiskLabel.values()).map(Enum::name)
).collect(Collectors.toSet());

boolean hasMeasureColumns = sqlRequest.getValueCols().stream()
    .map(columnVO -> columnVO.field())
    .anyMatch(field -> riskNames.stream().anyMatch(field::contains));
import java.util.stream.Collectors;

// Assuming sqlServerPaginationRequest is an object of a class that has the getColumnPivot() method
// and getColumnPivot() returns an object that has aggColumn(), pivotColumn(), and pivotValues() methods.

String aggColumn = sqlServerPaginationRequest.getColumnPivot().aggColumn();
String pivotColumn = sqlServerPaginationRequest.getColumnPivot().pivotColumn();
String pivotValues = sqlServerPaginationRequest.getColumnPivot().pivotValues()
                            .stream()
                            .map(value -> "'" + value + "'")
                            .collect(Collectors.joining(","));

String pivotQuery = String.format(
    "PIVOT (SUM(%s) FOR %s IN (%s))",
    aggColumn, pivotColumn, pivotValues
);

// return the pivot query
return pivotQuery;

import static org.junit.jupiter.api.Assertions.*;
import static org.mockito.Mockito.*;

import java.util.ArrayList;
import java.util.List;
import java.util.concurrent.ExecutionException;

import org.junit.jupiter.api.BeforeEach;
import org.junit.jupiter.api.Test;
import org.mockito.InjectMocks;
import org.mockito.Mock;
import org.mockito.MockitoAnnotations;

import com.google.api.core.ApiFuture;
import com.google.cloud.firestore.CollectionReference;
import com.google.cloud.firestore.Firestore;
import com.google.cloud.firestore.Query;
import com.google.cloud.firestore.QueryDocumentSnapshot;
import com.google.cloud.firestore.QuerySnapshot;

public class YourServiceTest {

    @Mock
    private Firestore firestore;

    @Mock
    private FirestoreConfig fireStoreConfig;

    @Mock
    private CollectionReference collectionReference;

    @Mock
    private Query query;

    @Mock
    private ApiFuture<QuerySnapshot> apiFuture;

    @Mock
    private QuerySnapshot querySnapshot;

    @InjectMocks
    private YourService yourService;

    @BeforeEach
    public void setUp() {
        MockitoAnnotations.openMocks(this);
        when(fireStoreConfig.instance()).thenReturn(firestore);
        when(firestore.collection(anyString())).thenReturn(collectionReference);
    }

    @Test
    public void shouldGetJobMetadataByJobId() throws Exception {
        String jobId = "testJobId";
        List<QueryDocumentSnapshot> documents = new ArrayList<>();
        QueryDocumentSnapshot documentSnapshot = mock(QueryDocumentSnapshot.class);
        PostRiskEngineRequestModel model = new PostRiskEngineRequestModel();
        
        documents.add(documentSnapshot);

        when(collectionReference.whereEqualTo("riskEngineJobUUID", jobId)).thenReturn(query);
        when(query.orderBy("version", Query.Direction.DESCENDING)).thenReturn(query);
        when(query.get()).thenReturn(apiFuture);
        when(apiFuture.get()).thenReturn(querySnapshot);
        when(querySnapshot.getDocuments()).thenReturn(documents);
        when(documentSnapshot.toObject(PostRiskEngineRequestModel.class)).thenReturn(model);

        PostRiskEngineRequestModel result = yourService.getJobMetadataByJobId(jobId);

        assertNotNull(result);
        assertEquals(model, result);
    }

    @Test
    public void shouldReturnNullWhenNoDocumentsFound() throws Exception {
        String jobId = "testJobId";
        List<QueryDocumentSnapshot> documents = new ArrayList<>();

        when(collectionReference.whereEqualTo("riskEngineJobUUID", jobId)).thenReturn(query);
        when(query.orderBy("version", Query.Direction.DESCENDING)).thenReturn(query);
        when(query.get()).thenReturn(apiFuture);
        when(apiFuture.get()).thenReturn(querySnapshot);
        when(querySnapshot.getDocuments()).thenReturn(documents);

        PostRiskEngineRequestModel result = yourService.getJobMetadataByJobId(jobId);

        assertNull(result);
    }

    @Test
    public void shouldReturnNullOnException() throws Exception {
        String jobId = "testJobId";

        when(collectionReference.whereEqualTo("riskEngineJobUUID", jobId)).thenReturn(query);
        when(query.orderBy("version", Query.Direction.DESCENDING)).thenReturn(query);
        when(query.get()).thenReturn(apiFuture);
        when(apiFuture.get()).thenThrow(new InterruptedException());

        PostRiskEngineRequestModel result = yourService.getJobMetadataByJobId(jobId);

        assertNull(result);
    }
}
